{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tempfile\n",
      "build_estimator\n",
      "model_dir: C:\\Users\\40712\\AppData\\Local\\Temp\\tmpoy7rxpam\n",
      "model_type: deep\n",
      "building deep model\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\40712\\\\AppData\\\\Local\\\\Temp\\\\tmpoy7rxpam', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016B022BF5F8>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "train start\n",
      "(7377418, 3)\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\40712\\AppData\\Local\\Temp\\tmpoy7rxpam\\model.ckpt.\n",
      "INFO:tensorflow:loss = 71.0419, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 20 into C:\\Users\\40712\\AppData\\Local\\Temp\\tmpoy7rxpam\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 61.4481.\n",
      "predict start\n",
      "(2556790, 3)\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\40712\\AppData\\Local\\Temp\\tmpoy7rxpam\\model.ckpt-20\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "CSV_COLUMNS = [\"msno\", \"song_id\", \"target\"]\n",
    "\n",
    "FEATURES = [\"msno\", \"song_id\"]\n",
    "\n",
    "LABEL = \"target\"\n",
    "\n",
    "msno = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "\n",
    "    \"msno\", hash_bucket_size=1000)\n",
    "\n",
    "song_id = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "\n",
    "    \"song_id\", hash_bucket_size=1000)\n",
    "\n",
    "deep_columns = [    \n",
    "\n",
    "    # To show an example of embedding\n",
    "\n",
    "    tf.feature_column.embedding_column(msno, dimension=8),\n",
    "\n",
    "    tf.feature_column.embedding_column(song_id, dimension=8)    \n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "def build_estimator(model_dir, model_type):\n",
    "\n",
    "    \"\"\"Build an estimator.\"\"\"\n",
    "    print('model_dir:',model_dir)\n",
    "    print('model_type:',model_type)\n",
    "\n",
    "    if model_type == \"wide\":\n",
    "\n",
    "        m = tf.estimator.LinearClassifier(\n",
    "\n",
    "            model_dir=model_dir, feature_columns=base_columns + crossed_columns)\n",
    "\n",
    "    elif model_type == \"deep\":\n",
    "        \n",
    "        print('building deep model')\n",
    "\n",
    "        m = tf.estimator.DNNClassifier(\n",
    "\n",
    "            model_dir=model_dir,\n",
    "\n",
    "            feature_columns=deep_columns,\n",
    "\n",
    "            hidden_units=[100, 50])\n",
    "\n",
    "    else:\n",
    "\n",
    "        m = tf.estimator.DNNLinearCombinedClassifier(\n",
    "\n",
    "            model_dir=model_dir,\n",
    "\n",
    "            linear_feature_columns=crossed_columns,\n",
    "\n",
    "            dnn_feature_columns=deep_columns,\n",
    "\n",
    "            dnn_hidden_units=[100, 50])\n",
    "\n",
    "    return m\n",
    "\n",
    "def input_fn(data_file, is_train, num_epochs, shuffle):\n",
    "\n",
    "    \"\"\"Input builder function.\"\"\"\n",
    "\n",
    "    df_data = pd.read_csv(\n",
    "\n",
    "      tf.gfile.Open(data_file),\n",
    "\n",
    "      names=CSV_COLUMNS,\n",
    "\n",
    "      skipinitialspace=True,\n",
    "\n",
    "      engine=\"python\",\n",
    "\n",
    "      skiprows=1)\n",
    "\n",
    "  # remove NaN elements\n",
    "    print(df_data.shape)\n",
    "    #df_data = df_data.dropna(how=\"any\", axis=0)    \n",
    "    df_data = df_data.fillna(value='unknown')    \n",
    "    \n",
    "    #labels = df_data[\"target\"]\n",
    "    #labels.head()\n",
    "    if is_train:\n",
    "        \n",
    "        return tf.estimator.inputs.pandas_input_fn(\n",
    "            \n",
    "          x = pd.DataFrame({k:df_data[k].values for k in FEATURES}),\n",
    "            \n",
    "          y = pd.Series(df_data[LABEL].values),     \n",
    "\n",
    "          batch_size=100,\n",
    "\n",
    "          num_epochs=num_epochs,\n",
    "\n",
    "          shuffle=shuffle,\n",
    "\n",
    "          num_threads=3)\n",
    "    else:\n",
    "        \n",
    "        return tf.estimator.inputs.pandas_input_fn(\n",
    "            \n",
    "          x = pd.DataFrame({k:df_data[k].values for k in FEATURES}),\n",
    "            \n",
    "          y = pd.Series(df_data[LABEL].values),     \n",
    "\n",
    "          batch_size=100,\n",
    "\n",
    "          num_epochs=num_epochs,\n",
    "\n",
    "          shuffle=shuffle,\n",
    "\n",
    "          num_threads=1)\n",
    "\n",
    "def train_and_eval(model_dir, model_type, train_steps, train_data, test_data):\n",
    "\n",
    "    \"\"\"Train and evaluate the model.\"\"\"\n",
    "\n",
    "    #train_file_name = 'dataset/train.csv/train1.csv'\n",
    "    #test_file_name = 'dataset/test.csv/test1.csv'\n",
    "\n",
    "  # Specify file path below if want to find the output easily\n",
    "    print('tempfile')\n",
    "    model_dir = tempfile.mkdtemp() if not model_dir else model_dir\n",
    "    \n",
    "\n",
    "    print('build_estimator')\n",
    "    m = build_estimator(model_dir, model_type)\n",
    "\n",
    "  # set num_epochs to None to get infinite stream of data.\n",
    "    \n",
    "    print('train start')\n",
    "    m.train(\n",
    "\n",
    "      input_fn=input_fn(train_data, True, num_epochs=1, shuffle=True),\n",
    "\n",
    "      steps=train_steps)\n",
    "\n",
    "  # set steps to None to run evaluation until all data consumed.\n",
    "    print('predict start')    \n",
    "    results = m.predict(input_fn = input_fn(test_data, False, num_epochs=1, shuffle=False))\n",
    "    #l = list(results)\n",
    "    #print(len(l))\n",
    "    #for r in results:\n",
    "        #print(np.argmax(r['probabilities']))\n",
    "    \n",
    "    with open(\"submission.csv\",'w',newline='') as f:\n",
    "        wr = csv.writer(f,dialect='excel')        \n",
    "        wr.writerow([\"id\",\"target\"])\n",
    "        i = 0\n",
    "        for r in results:\n",
    "            #print([i, np.argmax(r['probabilities'])])\n",
    "            wr.writerow([str(i), str(np.argmax(r['probabilities']))])\n",
    "            i+=1\n",
    "    \n",
    "    \n",
    "    #results = list(results.'probabilities')\n",
    "    #print('results:',results)\n",
    "    #for x in range(10):\n",
    "        #print(results[i])\n",
    "    #for i, p in enumerate(results):\n",
    "    #    print(\"Prediction %s: %s\" % (i + 1, p[\"targets\"]))\n",
    "    # Manual cleanup\n",
    "\n",
    "    shutil.rmtree(model_dir,ignore_errors=True)\n",
    "    print('end!')\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def main(_):\n",
    "\n",
    "    train_and_eval(FLAGS.model_dir, FLAGS.model_type, FLAGS.train_steps,\n",
    "\n",
    "                 FLAGS.train_data, FLAGS.test_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "\n",
    "    parser.add_argument(\n",
    "\n",
    "      \"--model_dir\",\n",
    "\n",
    "      type=str,\n",
    "\n",
    "      default=\"\",\n",
    "\n",
    "      help=\"Base directory for output models.\"\n",
    "\n",
    "  )\n",
    "\n",
    "    parser.add_argument(\n",
    "\n",
    "      \"--model_type\",\n",
    "\n",
    "      type=str,\n",
    "\n",
    "      default=\"deep\",\n",
    "\n",
    "      help=\"Valid model types: {'wide', 'deep', 'wide_n_deep'}.\"\n",
    "\n",
    "  )\n",
    "\n",
    "    parser.add_argument(\n",
    "\n",
    "      \"--train_steps\",\n",
    "\n",
    "      type=int,\n",
    "\n",
    "      default=20,\n",
    "\n",
    "      help=\"Number of training steps.\"\n",
    "\n",
    "  )\n",
    "\n",
    "    parser.add_argument(\n",
    "\n",
    "      \"--train_data\",\n",
    "\n",
    "      type=str,\n",
    "\n",
    "      default=\"dataset/train.csv/train.csv\",\n",
    "\n",
    "      help=\"Path to the training data.\"\n",
    "\n",
    "  )\n",
    "\n",
    "    parser.add_argument(\n",
    "\n",
    "      \"--test_data\",\n",
    "\n",
    "      type=str,\n",
    "\n",
    "      default=\"dataset/test.csv/test.csv\",\n",
    "\n",
    "      help=\"Path to the test data.\"\n",
    "\n",
    "  )\n",
    "\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-abf77d3f21d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM='\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc='\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "x = ['V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM=','WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc=']\n",
    "y = m.predict(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"test.csv\",\"w\",newline='') as csvfile: \n",
    "    writer = csv.writer(csvfile,dialect='excel')\n",
    "    #先写入columns_name\n",
    "    writer.writerow([\"index\",\"a_name\",\"b_name\"])\n",
    "    #写入多行用writerows\n",
    "    writer.writerows([[0,1,3],[1,2,3],[2,3,4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 6)\n",
      "(8, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('dataset/test.csv/test1.csv')\n",
    "s = pd.read_csv('submission.csv')\n",
    "print(test.shape)\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
