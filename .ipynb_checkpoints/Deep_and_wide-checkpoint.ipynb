{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tempfile\n",
      "build_estimator\n",
      "model_dir: C:\\Users\\40712\\AppData\\Local\\Temp\\tmpoy7rxpam\n",
      "model_type: deep\n",
      "building deep model\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\40712\\\\AppData\\\\Local\\\\Temp\\\\tmpoy7rxpam', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016B022BF5F8>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "train start\n",
      "(7377418, 3)\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\40712\\AppData\\Local\\Temp\\tmpoy7rxpam\\model.ckpt.\n",
      "INFO:tensorflow:loss = 71.0419, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 20 into C:\\Users\\40712\\AppData\\Local\\Temp\\tmpoy7rxpam\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 61.4481.\n",
      "predict start\n",
      "(2556790, 3)\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\40712\\AppData\\Local\\Temp\\tmpoy7rxpam\\model.ckpt-20\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "CSV_COLUMNS = \"1000,1007,1011,1011|2189|367,1011|359,1011|691,1019,1026,1033,1040,1040|1155,1047,1054,1068,1082,\\\n",
    "109,1096,1096|958,109|1138,109|118,109|1259,109|139,109|2122,109|359,109|465,109|921,109|94,1103,1110,1117,1124,\\\n",
    "1131,1138,1138|2122,1145,1145|1103,1145|1110,1145|2122,1145|423,1152,1152|1609,1152|1609|921|2122|1259|958,\\\n",
    "1152|2022,1152|2122,1152|2122|786|947,1152|2130|2086|374|2122,1152|242,1152|242|947,1152|359,1152|388,1152|423,\\\n",
    "1152|458,1152|465,1152|465|1180,1152|465|958,1152|691,1152|726,1152|786,1152|786|2086|374|2122|958,1152|786|947,\\\n",
    "1152|786|947|2086|374|2122|958,1152|829,1152|873,1152|873|2122|958,1152|873|726|958,1152|873|947,1152|921,1152|940,\\\n",
    "1152|947,1152|947|2086|374|2122|958,1152|947|958,1155,1162,1169,118,1180,1180|1152,1180|1259,1180|139,1180|2022,\\\n",
    "1180|2219,1180|437,1180|465,1187,118|1259,1194,1201,1201|1187,1208,1208|423,1259,1259|139,1259|139|125|109,1259|359,\\\n",
    "125|95,1266,1273,1280,1280|1145,1280|2093,1287,139,139|1259,139|1259|125|109,139|125|109,139|125|109|2022,139|125|95,139|359,139|444|109,139|691,152,1568,1568|2015,1568|465,1572,1572|2065,1572|275,1579,1598,1605,1609,1609|1259,1609|139,1609|139|125|109,1609|1969|2100,1609|2022,1609|2058,1609|2086|374,1609|2107,1609|2122,1609|2122|2022,1609|2122|2022|958,1609|2122|786,1609|2122|958,1609|242,1609|275|1572,1609|282,1609|359,1609|458,1609|465,1609|509,1609|786,1609|921,1609|940|2086,1609|947,1609|947|2022,1609|947|2086|374,1609|947|726|2022,1609|958,1616,1616|1609,1616|2058,1616|2065,1616|2072,1616|2109,1616|2116,1616|2176,1630,1633,1633|2022,1633|310,1633|359,1633|465,177,184,184|2122,191,1944,1944|310,1955,1965,1969,1969|1609|2100,1969|2100|2022,1969|275|2100|1572,1969|444|2100,1977,198,1981,1988|1981|430,1988|430,198|2122,1995,2008|296,2015,2022,2022|1011,2022|1259,2022|1609|139|125|109,2022|1955,2022|2122,2022|2215,2022|242,2022|359,2022|430,2022|451,2022|531,2022|691,2022|786,2022|947,2022|958,2029,2032,205,2052,2065,2072,2079,2086,2086|374,2086|374|139|125|109,2093,2100,2100|1969,2107,2109,2116|1609,2116|1616,212,2122,2122|1259,2122|139,2122|139|125|109,2122|191,2122|2086|374,2122|2189|367,2122|374,2122|423,2122|691,2122|786,2122|786|947|2022,2122|786|947|958,2122|947,2122|947|2022|958,2122|947|726,2122|947|958,2122|947|958|2022,2122|958,2127,2130,2130|2022,2130|2122,2130|2122|958,2130|947|2086|374|2122|958,2130|947|2122|139|125|109|798|958,2144,2144|458,2150,2157,2157|1259,2157|921,2172,2176,2176|2093,2183,2189,2189|367,2189|798,2189|958|367,2192,2194,2206,2213|465,2215,2245,2245|242,2248|1259,242,242|2022,242|691,242|726,242|786,242|947,252,275|1572|2022,275|1572|359,275|1572|444,275|1572|829,275|1955|1572,275|531|1572,331,338,352,352|1995,352|275|1995|1572,359,359|1259,359|139,374,381|2086,381|2086|374,388,388|1011,388|1152,388|139,388|2122,388|2189|367,388|2189|958|367,388|465,388|509,388|691,388|786,388|873,388|921,388|940,402,409,409|465,409|958,416,423,423|531,430,430|1011,430|139,430|359,430|691,437,437|1287,437|139,437|2022,437|359,437|850,444,444|1259,444|139,444|1609,444|2022,444|359,444|465,444|786,444|829,444|921,444|958,451,451|1259,451|2189|367,458,458|1287,458|139|109,458|2130,458|359,458|531,458|786,465,465|1011,465|1011|691,465|109,465|1103,465|1138,465|1145,465|1145|2122,465|1180|2189|367,465|1259,465|1259|1180,465|139,465|139|109,465|139|1259,465|139|125|109,465|139|94,465|139|958,465|1609,465|1609|1103,465|1955|2022,465|2022,465|2022|359,465|2022|958,465|2122,465|2122|109,465|2122|139,465|2122|423,465|2130,465|2130|139,465|2189|367,465|2213|2215,465|242,465|359,465|388|958,465|409,465|423,465|430,465|451,465|458,465|531,465|691,465|691|2189|367,465|726,465|786,465|798,465|829,465|873,465|921,465|921|139,465|921|2122,465|94,465|947,465|958,465|958|2022,474,481,488,502,509,516,516|465,531,545,545|1259,545|2022,545|430,545|726,649,656,656|1011,670,677,691,698,712,719,726,726|2122,726|242,726|458,726|691,726|947,726|958,744,751,779,786,786|1011,786|1259,786|139|125|109,786|2022,786|2086|374,786|2122,786|2122|947|2022,786|2122|947|958,786|242,786|359,786|691,786|726,786|798,786|947,786|947|1609|2122|958,786|947|2022|2122|958,786|947|2086|374,786|947|2086|374|2122,786|947|2086|374|958,786|947|2122|139|125|109|958,786|947|726,786|947|726|958,786|947|958,786|958,798,798|2022,798|786,808,822,829,829|1011,829|1138,829|2022,829|2130,829|242,829|359,829|430,829|437,829|458,829|726,829|786,829|798,829|822,843,850,864|465|242|850|1609|857|843,864|465|850|857|843,864|786|850|857|843,864|843,864|850|2022|857|843,864|850|726|857|843,864|850|857|2122|843,864|850|857|921|843,864|857|850|843,87,873,873|1259,873|139|125|109,873|2022,873|2086|374,873|2122,873|465\\\n",
    ",873|726,873|786,873|947,873|958,880,880|102,880|430,880|458,880|465,880|545,880|726,880|786,880|873,880|873|786,\\\n",
    "880|873|947,893,893|458,900,900|465,900|921,900|958,907,921,921|1011,921|1259,921|139,921|1609,\\\n",
    "921|1609|786,921|1633,921|2022,921|2122,921|2130,921|2206,921|359,921|423,921|451,921|458,921|465,921|465|139,\\\n",
    "921|465|2022,921|465|958,921|726,921|786,921|798,921|873,921|893,921|900|893,921|947,921|947|2086|374,921|958,94,\\\n",
    "940,940|1152,940|1609,940|388,940|726,940|786,947,947|139|125|109,947|2022|2122|1259|958,947|2022|958,\\\n",
    "947|2122|139|125|109,947|2122|139|125|109|958,947|726|958,947|958|2022,94|139,94|95,95,958,958|2022,958|2122,\\\n",
    "958|691,958|786,958|947,965,972,979,986,993|751,,discover,explore,listen with,my library,notification,\\\n",
    "radio,search,settings,Album more,Artist more,Concert,Discover Chart,Discover Feature,Discover Genre,Discover New,\\\n",
    "Explore,Local playlist more,My library,My library_Search,Online playlist more,Others profile more,Payment,Radio,\\\n",
    "Search,Search Home,Search Trends,Self profile more,Unknown,,album,artist,listen-with,local-library,local-playlist,\\\n",
    "my-daily-playlist,online-playlist,radio,song,song-based-playlist,top-hits-for-artist,topic-article-playlist,\\\n",
    ",female,male,,1.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,\\\n",
    ",-1.0,3.0,10.0,17.0,24.0,31.0,38.0,45.0,52.0,59.0,,target\".split()\n",
    "\n",
    "FEATURES = CSV_COLUMNS[:-1]\n",
    "\n",
    "LABEL = CSV_COLUMNS[-1]\n",
    "base_columns = FEATURES\n",
    "crossed_columns = []\n",
    "\n",
    "msno = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "\n",
    "    \"msno\", hash_bucket_size=1000)\n",
    "\n",
    "song_id = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "\n",
    "    \"song_id\", hash_bucket_size=1000)\n",
    "\n",
    "deep_columns = [    \n",
    "\n",
    "    # To show an example of embedding\n",
    "\n",
    "    tf.feature_column.embedding_column(msno, dimension=8),\n",
    "\n",
    "    tf.feature_column.embedding_column(song_id, dimension=8)    \n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "def build_estimator(model_dir, model_type):\n",
    "\n",
    "    \"\"\"Build an estimator.\"\"\"\n",
    "    print('model_dir:',model_dir)\n",
    "    print('model_type:',model_type)\n",
    "\n",
    "    if model_type == \"wide\":\n",
    "\n",
    "        m = tf.estimator.LinearClassifier(\n",
    "\n",
    "            model_dir=model_dir, feature_columns=base_columns + crossed_columns)\n",
    "\n",
    "    elif model_type == \"deep\":\n",
    "        \n",
    "        print('building deep model')\n",
    "\n",
    "        m = tf.estimator.DNNClassifier(\n",
    "\n",
    "            model_dir=model_dir,\n",
    "\n",
    "            feature_columns=deep_columns,\n",
    "\n",
    "            hidden_units=[100, 50])\n",
    "\n",
    "    else:\n",
    "\n",
    "        m = tf.estimator.DNNLinearCombinedClassifier(\n",
    "\n",
    "            model_dir=model_dir,\n",
    "\n",
    "            linear_feature_columns=crossed_columns,\n",
    "\n",
    "            dnn_feature_columns=deep_columns,\n",
    "\n",
    "            dnn_hidden_units=[100, 50])\n",
    "\n",
    "    return m\n",
    "\n",
    "def input_fn(data_file, is_train, num_epochs, shuffle):\n",
    "\n",
    "    \"\"\"Input builder function.\"\"\"\n",
    "\n",
    "    df_data = pd.read_csv(\n",
    "\n",
    "      tf.gfile.Open(data_file),\n",
    "\n",
    "      names=CSV_COLUMNS,\n",
    "\n",
    "      skipinitialspace=True,\n",
    "\n",
    "      engine=\"python\",\n",
    "\n",
    "      skiprows=1)\n",
    "\n",
    "  # remove NaN elements\n",
    "    print(df_data.shape)\n",
    "    #df_data = df_data.dropna(how=\"any\", axis=0)    \n",
    "    df_data = df_data.fillna(value='unknown')    \n",
    "    \n",
    "    #labels = df_data[\"target\"]\n",
    "    #labels.head()\n",
    "    if is_train:\n",
    "        \n",
    "        return tf.estimator.inputs.pandas_input_fn(\n",
    "            \n",
    "          x = pd.DataFrame({k:df_data[k].values for k in FEATURES}),\n",
    "            \n",
    "          y = pd.Series(df_data[LABEL].values),     \n",
    "\n",
    "          batch_size=100,\n",
    "\n",
    "          num_epochs=num_epochs,\n",
    "\n",
    "          shuffle=shuffle,\n",
    "\n",
    "          num_threads=3)\n",
    "    else:\n",
    "        \n",
    "        return tf.estimator.inputs.pandas_input_fn(\n",
    "            \n",
    "          x = pd.DataFrame({k:df_data[k].values for k in FEATURES}),\n",
    "            \n",
    "          y = pd.Series(df_data[LABEL].values),     \n",
    "\n",
    "          batch_size=100,\n",
    "\n",
    "          num_epochs=num_epochs,\n",
    "\n",
    "          shuffle=shuffle,\n",
    "\n",
    "          num_threads=1)\n",
    "\n",
    "def train_and_eval(model_dir, model_type, train_steps, train_data, test_data):\n",
    "\n",
    "    \"\"\"Train and evaluate the model.\"\"\"\n",
    "\n",
    "    #train_file_name = 'dataset/train.csv/train1.csv'\n",
    "    #test_file_name = 'dataset/test.csv/test1.csv'\n",
    "\n",
    "  # Specify file path below if want to find the output easily\n",
    "    print('tempfile')\n",
    "    model_dir = tempfile.mkdtemp() if not model_dir else model_dir\n",
    "    \n",
    "\n",
    "    print('build_estimator')\n",
    "    m = build_estimator(model_dir, model_type)\n",
    "\n",
    "  # set num_epochs to None to get infinite stream of data.\n",
    "    \n",
    "    print('train start')\n",
    "    m.train(\n",
    "\n",
    "      input_fn=input_fn(train_data, True, num_epochs=1, shuffle=True),\n",
    "\n",
    "      steps=train_steps)\n",
    "\n",
    "  # set steps to None to run evaluation until all data consumed.\n",
    "    print('predict start')    \n",
    "    results = m.predict(input_fn = input_fn(test_data, False, num_epochs=1, shuffle=False))\n",
    "    #l = list(results)\n",
    "    #print(len(l))\n",
    "    #for r in results:\n",
    "        #print(np.argmax(r['probabilities']))\n",
    "    \n",
    "    with open(\"submission.csv\",'w',newline='') as f:\n",
    "        wr = csv.writer(f,dialect='excel')        \n",
    "        wr.writerow([\"id\",\"target\"])\n",
    "        i = 0\n",
    "        for r in results:\n",
    "            #print([i, np.argmax(r['probabilities'])])\n",
    "            wr.writerow([str(i), str(np.argmax(r['probabilities']))])\n",
    "            i+=1\n",
    "    \n",
    "    \n",
    "    #results = list(results.'probabilities')\n",
    "    #print('results:',results)\n",
    "    #for x in range(10):\n",
    "        #print(results[i])\n",
    "    #for i, p in enumerate(results):\n",
    "    #    print(\"Prediction %s: %s\" % (i + 1, p[\"targets\"]))\n",
    "    # Manual cleanup\n",
    "\n",
    "    shutil.rmtree(model_dir,ignore_errors=True)\n",
    "    print('end!')\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def main(_):\n",
    "\n",
    "    train_and_eval(FLAGS.model_dir, FLAGS.model_type, FLAGS.train_steps,\n",
    "\n",
    "                 FLAGS.train_data, FLAGS.test_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "\n",
    "    parser.add_argument(\n",
    "\n",
    "      \"--model_dir\",\n",
    "\n",
    "      type=str,\n",
    "\n",
    "      default=\"\",\n",
    "\n",
    "      help=\"Base directory for output models.\"\n",
    "\n",
    "  )\n",
    "\n",
    "    parser.add_argument(\n",
    "\n",
    "      \"--model_type\",\n",
    "\n",
    "      type=str,\n",
    "\n",
    "      default=\"wide\",\n",
    "\n",
    "      help=\"Valid model types: {'wide', 'deep', 'wide_n_deep'}.\"\n",
    "\n",
    "  )\n",
    "\n",
    "    parser.add_argument(\n",
    "\n",
    "      \"--train_steps\",\n",
    "\n",
    "      type=int,\n",
    "\n",
    "      default=20,\n",
    "\n",
    "      help=\"Number of training steps.\"\n",
    "\n",
    "  )\n",
    "\n",
    "    parser.add_argument(\n",
    "\n",
    "      \"--train_data\",\n",
    "\n",
    "      type=str,\n",
    "\n",
    "      default=\"./train.csv\",\n",
    "\n",
    "      help=\"Path to the training data.\"\n",
    "\n",
    "  )\n",
    "\n",
    "    parser.add_argument(\n",
    "\n",
    "      \"--test_data\",\n",
    "\n",
    "      type=str,\n",
    "\n",
    "      default=\"./valid.csv\",\n",
    "\n",
    "      help=\"Path to the test data.\"\n",
    "\n",
    "  )\n",
    "\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-abf77d3f21d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM='\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc='\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "x = ['V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM=','WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc=']\n",
    "y = m.predict(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"test.csv\",\"w\",newline='') as csvfile: \n",
    "    writer = csv.writer(csvfile,dialect='excel')\n",
    "    #先写入columns_name\n",
    "    writer.writerow([\"index\",\"a_name\",\"b_name\"])\n",
    "    #写入多行用writerows\n",
    "    writer.writerows([[0,1,3],[1,2,3],[2,3,4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 6)\n",
      "(8, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('dataset/test.csv/test1.csv')\n",
    "s = pd.read_csv('submission.csv')\n",
    "print(test.shape)\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
