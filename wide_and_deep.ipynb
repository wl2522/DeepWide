{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Load data and reduce the number of features for the baseline models\n",
    "\n",
    "train = pd.read_csv('../../wl2522/project/dataset/train.csv', dtype={'source_system_tab': str})\n",
    "test = pd.read_csv('../../wl2522/project/dataset/test.csv', dtype={'source_system_tab': str })\n",
    "members = pd.read_csv('../../wl2522/project/dataset/members.csv', dtype={'msno': str, 'city': str, 'registered_via': str})\n",
    "songs = pd.read_csv('../../wl2522/project/dataset/songs.csv', dtype={'genre_ids': str, 'language': str, 'song_length': int})\n",
    "\n",
    "#Infer a missing value based on other features\n",
    "\n",
    "songs.loc[605127, 'language'] = '31.0'\n",
    "\n",
    "#Impute missing values\n",
    "\n",
    "train.fillna(value='unknown', axis=1, inplace=True)\n",
    "test.fillna(value='unknown', axis=1, inplace=True)\n",
    "members.fillna(value='unknown', axis=1, inplace=True)\n",
    "songs.fillna(value='unknown', axis=1, inplace=True)\n",
    "\n",
    "#Convert feature columns related to member registration to datetime format\n",
    "\n",
    "members['expiration_date'] = pd.to_datetime(members['expiration_date'], format='%Y%m%d')\n",
    "members['registration_init_time'] = pd.to_datetime(members['registration_init_time'], format='%Y%m%d')\n",
    "\n",
    "#Create a feature indicating the number of days a member was registered\n",
    "\n",
    "members['reg_duration'] = (members['expiration_date'] - members['registration_init_time']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create lists of genre ids in the genre id column\n",
    "\n",
    "#genres = songs['genre_ids'].str.split('|')\n",
    "\n",
    "#Create a dataframe that stores genre IDs across multiple columns (one genre per column)\n",
    "\n",
    "#genres = genres.apply(pd.Series).add_prefix('genre_')\n",
    "#genres.to_csv('dataset/genres.csv', index=False)\n",
    "\n",
    "genres = pd.read_csv('../../wl2522/project/dataset/genres.csv', dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Merge the training and test data with song and member data\n",
    "\n",
    "train_set = train.merge(songs, on='song_id')\n",
    "train_set = train_set.merge(members, on='msno')\n",
    "test_set = test.merge(songs, on='song_id', how='left')\n",
    "test_set = test_set.merge(members, on='msno', how='left')\n",
    "\n",
    "#Separate the submission ids from the test set\n",
    "\n",
    "ids = test_set['id']\n",
    "test_set.drop('id', axis=1, inplace=True)\n",
    "\n",
    "#Impute missing values for specific features in the merged training and test sets with an integer value \n",
    "\n",
    "msno_vocabs = train_set['msno'].unique()\n",
    "msno_map = { msno_vocabs[i]:i+1 for i in range(len(msno_vocabs))}\n",
    "train_set['msno'] = train_set['msno'].map(msno_map).fillna(-99).astype(int)\n",
    "test_set['msno'] = test_set['msno'].map(msno_map).fillna(-99).astype(int)\n",
    "\n",
    "song_vocabs = train_set['song_id'].unique()\n",
    "song_map = { song_vocabs[i]:i+1 for i in range(len(song_vocabs))}\n",
    "train_set['song_id'] = train_set['song_id'].map(song_map).fillna(-99).astype(int)\n",
    "test_set['song_id'] = test_set['song_id'].map(song_map).fillna(-99).astype(int)\n",
    "\n",
    "tab_vocabs = train_set['source_system_tab'].unique()\n",
    "tab_map = { tab_vocabs[i]:i+1 for i in range(len(tab_vocabs))}\n",
    "train_set['source_system_tab'] = train_set['source_system_tab'].map(tab_map).fillna(-99).astype(int)\n",
    "test_set['source_system_tab'] = test_set['source_system_tab'].map(tab_map).fillna(-99).astype(int)\n",
    "\n",
    "screen_vocabs = train_set['source_screen_name'].unique()\n",
    "screen_map = { screen_vocabs[i]:i+1 for i in range(len(screen_vocabs))}\n",
    "train_set['source_screen_name'] = train_set['source_screen_name'].map(screen_map).fillna(-99).astype(int)\n",
    "test_set['source_screen_name'] = test_set['source_screen_name'].map(screen_map).fillna(-99).astype(int)\n",
    "\n",
    "sctype_vocabs = train_set['source_type'].unique()\n",
    "sctype_map = { sctype_vocabs[i]:i+1 for i in range(len(sctype_vocabs))}\n",
    "train_set['source_type'] = train_set['source_type'].map(sctype_map).fillna(-99).astype(int)\n",
    "test_set['source_type'] = test_set['source_type'].map(sctype_map).fillna(-99).astype(int)\n",
    "\n",
    "gid_vocabs = train_set['genre_ids'].unique()\n",
    "gid_map = { gid_vocabs[i]:i+1 for i in range(len(gid_vocabs))}\n",
    "train_set['genre_ids'] = train_set['genre_ids'].map(gid_map).fillna(-99).astype(int)\n",
    "test_set['genre_ids'] = test_set['genre_ids'].map(gid_map).fillna(-99).astype(int)\n",
    "\n",
    "gender_vocabs = train_set['gender'].unique()\n",
    "gender_map = { gender_vocabs[i]:i+1 for i in range(len(gender_vocabs))}\n",
    "train_set['gender'] = train_set['gender'].map(gender_map).fillna(-99).astype(int)\n",
    "test_set['gender'] = test_set['gender'].map(gender_map).fillna(-99).astype(int)\n",
    "\n",
    "city_vocabs = train_set['city'].unique()\n",
    "city_map = { city_vocabs[i]:i+1 for i in range(len(city_vocabs))}\n",
    "train_set['city'] = train_set['city'].map(city_map).fillna(-99).astype(int)\n",
    "test_set['city'] = test_set['city'].map(city_map).fillna(-99).astype(int)\n",
    "\n",
    "lan_vocabs = train_set['language'].unique()\n",
    "lan_map = { lan_vocabs[i]:i+1 for i in range(len(lan_vocabs))}\n",
    "train_set['language'] = train_set['language'].map(lan_map).fillna(-99).astype(int)\n",
    "test_set['language'] = test_set['language'].map(lan_map).fillna(-99).astype(int)\n",
    "\n",
    "#Impute missing song lengths with an integer value to avoid errors due to conflicting data types\n",
    "\n",
    "test_set['song_length'] = test_set['song_length'].replace('unknown', -99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "#load the pretrained GBDT model in order to use its predictions as a feature in the final model\n",
    "\n",
    "train_wide = pd.read_csv('tmp/train.csv', index_col=0)\n",
    "test_wide = pd.read_csv('tmp/test.csv', index_col=0)\n",
    "train_wide.drop('target', axis=1, inplace=True)\n",
    "\n",
    "feature_trans = joblib.load('./tmp/lgb.pkl')\n",
    "tr_features = feature_trans.predict(train_wide.values, pred_leaf=True)\n",
    "tes_features = feature_trans.predict(test_wide.values, pred_leaf=True)\n",
    "\n",
    "width = tr_features.shape[1]\n",
    "trans_feature = ['trans_feature' + str(i) for i in range(width)]\n",
    "train_trans = pd.DataFrame()\n",
    "test_trans = pd.DataFrame()\n",
    "\n",
    "for i in range(len(trans_feature)):\n",
    "    train_trans[trans_feature[i]] = tr_features[:, i]\n",
    "    test_trans[trans_feature[i]] = tes_features[:, i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Concatenate the original dataset with the GBDT model predictions\n",
    "\n",
    "train_dw = pd.concat([train_set, train_trans], axis=1)\n",
    "test_dw = pd.concat([test_set, test_trans], axis=1)\n",
    "\n",
    "#Impute values for missing song data and gender\n",
    "\n",
    "train_dw['song_length'] = pd.to_numeric(train_dw['song_length'],\n",
    "                                        downcast='integer', errors='coerce').fillna(0)\n",
    "test_dw['song_length'] = pd.to_numeric(test_dw['song_length'],\n",
    "                                       downcast='integer', errors='coerce').fillna(0)\n",
    "\n",
    "test_dw['gender'] = test_dw['gender'].fillna(-99)\n",
    "train_dw['gender'] = train_dw['gender'].fillna(-99)\n",
    "test_dw['gender'] = test_dw['gender'].astype(int)\n",
    "train_dw['gender'] = train_dw['gender'].astype(int)\n",
    "\n",
    "#Convert the song length and gender values to integer to avoid data type errors\n",
    "\n",
    "train_dw['song_length'] = train_dw['song_length'].astype('int32')\n",
    "test_dw['song_length'] = test_dw['song_length'].astype('int32')\n",
    "\n",
    "train_dw.to_csv('tmp/train_dw.csv')\n",
    "test_dw.to_csv('tmp/test_dw.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dw = pd.read_csv('tmp/train_dw.csv')\n",
    "test_dw = pd.read_csv('tmp/test_dw.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert feature column values back to integers for compatibility with the feature column API\n",
    "\n",
    "features = ['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type',\n",
    "            'genre_ids', 'language', 'city', 'bd', 'gender', 'reg_duration']\n",
    "\n",
    "for column in features:\n",
    "    train_dw[column] = train_dw[column].fillna(-99)\n",
    "    train_dw[column] = train_dw[column].astype(int)\n",
    "    \n",
    "for column in features:\n",
    "        test_dw[column] = test_dw[column].fillna(-99)\n",
    "        test_dw[column] = test_dw[column].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Shuffle the data and split off 20% of the training set for use as a validation set\n",
    "\n",
    "split_ratio = 0.8\n",
    "\n",
    "train_dw = train_dw.sample(frac=1, random_state=6)\n",
    "val_dw = train_dw[int(split_ratio*train_dw.shape[0]):]\n",
    "train_dw = train_dw[:int(split_ratio*train_dw.shape[0])]\n",
    "\n",
    "#Separate the labels from the training and validation sets\n",
    "\n",
    "y_train = train_dw['target']\n",
    "train_dw.drop('target', axis=1, inplace=True)\n",
    "\n",
    "y_val = val_dw['target']\n",
    "val_dw.drop('target', axis=1, inplace=True)\n",
    "\n",
    "train_set = train_dw\n",
    "test_set = test_dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Designate the target feature name and the features to be used in the dataset\n",
    "\n",
    "FEATURES = ['msno', 'gender', 'city', 'bd',\n",
    "            'song_id', 'language', 'genre_ids', 'composer', 'lyricist',\n",
    "            'source_system_tab', 'source_screen_name', 'source_type', 'reg_duration']\n",
    "\n",
    "#Include the GBDT predictions as features in the final model (one for each tree in the GBDT model, 32 leaves per tree)\n",
    "\n",
    "width = 60\n",
    "trans_feature = ['trans_feature' + str(i) for i in range(width)]\n",
    "FEATURES.extend(trans_feature)\n",
    "\n",
    "#Use the feature_column module to input each feature column into the model\n",
    "\n",
    "target = tf.feature_column.categorical_column_with_identity(key='target', num_buckets=2)\n",
    "\n",
    "length = tf.feature_column.numeric_column(key='song_length',\n",
    "                                          default_value=-1,\n",
    "                                          dtype=tf.int32)\n",
    "\n",
    "duration = tf.feature_column.numeric_column(key='reg_duration',\n",
    "                                            default_value=-1,\n",
    "                                            dtype=tf.int32)\n",
    "\n",
    "gender = tf.feature_column.categorical_column_with_vocabulary_list(key='gender',\n",
    "                                                                   vocabulary_list=train_dw['gender'].unique(),\n",
    "                                                                   dtype=tf.int32,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "city = tf.feature_column.categorical_column_with_vocabulary_list(key='city',\n",
    "                                                          vocabulary_list=train_dw['city'].unique(),\n",
    "                                                          dtype=tf.int32,\n",
    "                                                          default_value=-99)\n",
    "\n",
    "language = tf.feature_column.categorical_column_with_vocabulary_list(key='language',\n",
    "                                                                     vocabulary_list=train_dw['language'].unique(),\n",
    "                                                                     dtype=tf.int32,\n",
    "                                                                     default_value=-99)\n",
    "\n",
    "composer = tf.feature_column.categorical_column_with_vocabulary_list(key='composer',\n",
    "                                                                     vocabulary_list=songs['composer'].unique(),\n",
    "                                                                     dtype=tf.string,\n",
    "                                                                     default_value=-99)\n",
    "\n",
    "lyricist = tf.feature_column.categorical_column_with_vocabulary_list(key='lyricist',\n",
    "                                                                     vocabulary_list=songs['lyricist'].unique(),\n",
    "                                                                     dtype=tf.string,\n",
    "                                                                     default_value=-99)\n",
    "\n",
    "tab = tf.feature_column.categorical_column_with_vocabulary_list(key='source_system_tab',\n",
    "                                                                vocabulary_list=train_set['source_system_tab'].unique(),\n",
    "                                                                dtype=tf.int32,\n",
    "                                                                default_value=-99)\n",
    "\n",
    "screen = tf.feature_column.categorical_column_with_vocabulary_list(key='source_screen_name',\n",
    "                                                                   vocabulary_list=train_set['source_screen_name'].unique(),\n",
    "                                                                   dtype=tf.int32,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "source = tf.feature_column.categorical_column_with_vocabulary_list(key='source_type',\n",
    "                                                                   vocabulary_list=train_set['source_type'].unique(),\n",
    "                                                                   dtype=tf.int32,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "#Bucket categorical features with many unique categories using a hash table with a size of approximately (n/0.8)*2\n",
    "\n",
    "msno = tf.feature_column.categorical_column_with_hash_bucket(key='msno',\n",
    "                                                               hash_bucket_size=90000,\n",
    "                                                               dtype=tf.int32)\n",
    "\n",
    "song_id = tf.feature_column.categorical_column_with_hash_bucket(key='song_id',\n",
    "                                                             hash_bucket_size=6000000,\n",
    "                                                             dtype=tf.int32)\n",
    "\n",
    "hashed_genre = tf.feature_column.categorical_column_with_hash_bucket(key='genre_ids',\n",
    "                                                                     hash_bucket_size=3000,\n",
    "                                                                     dtype=tf.int32)\n",
    "\n",
    "#Perform one hot encoding on categorical features with few unique values\n",
    "\n",
    "indicator_gender = tf.feature_column.indicator_column(gender)\n",
    "indicator_city = tf.feature_column.indicator_column(city)\n",
    "indicator_language = tf.feature_column.indicator_column(language)\n",
    "indicator_tab = tf.feature_column.indicator_column(tab)\n",
    "indicator_screen = tf.feature_column.indicator_column(screen)\n",
    "indicator_source = tf.feature_column.indicator_column(source)\n",
    "\n",
    "#Embed the categorical feature with <100 unique categories into dense vectors with approximately log2(n) dimensions\n",
    "\n",
    "embedded_genre = tf.feature_column.embedding_column(hashed_genre, dimension=10)\n",
    "embedded_song = tf.feature_column.embedding_column(song_id, dimension=22)\n",
    "embedded_msno = tf.feature_column.embedding_column(msno, dimension=15)\n",
    "embedded_composer = tf.feature_column.embedding_column(composer, dimension=18)\n",
    "embedded_lyricist = tf.feature_column.embedding_column(lyricist, dimension=17)\n",
    "\n",
    "#Bucket member age into age ranges, with nonsensical values going into the 0-14 or the >80 buckets\n",
    "\n",
    "age = tf.feature_column.numeric_column(key='bd',\n",
    "                                       default_value=0,\n",
    "                                       dtype=tf.int32)\n",
    "\n",
    "age_bucket = tf.feature_column.bucketized_column(age, boundaries=[0, 14, 20, 30, 40, 50, 80])\n",
    "\n",
    "#Assign features to be used in either the wide or the deep model\n",
    "\n",
    "wide_columns =  [tf.feature_column.categorical_column_with_vocabulary_list(key='trans_feature' + str(i),\n",
    "                                                                   vocabulary_list=list(range(2**5)),\n",
    "                                                                   dtype=tf.int32,\n",
    "                                                                   default_value=-99) for i in range(60)]\n",
    "\n",
    "deep_columns = [indicator_gender, indicator_city, indicator_language,\n",
    "                indicator_tab, indicator_screen, indicator_source,\n",
    "                embedded_genre, embedded_msno, embedded_song,\n",
    "                embedded_composer, embedded_lyricist,\n",
    "                duration, age_bucket]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create an instance of the TensorFlow estimator for the specified model\n",
    "\n",
    "def build_estimator(model_dir, model_type):\n",
    "    if model_type == 'wide':\n",
    "        model = tf.estimator.LinearClassifier(model_dir=model_dir,\n",
    "                                              feature_columns=wide_columns)\n",
    "\n",
    "    elif model_type == 'deep':\n",
    "        model = tf.estimator.DNNClassifier(model_dir=model_dir,\n",
    "                                           feature_columns=deep_columns,\n",
    "                                           hidden_units=[1024, 512, 256],\n",
    "                                           optimizer=tf.train.AdamOptimizer(learning_rate=0.001,\n",
    "                                                                            name='Adam'),\n",
    "                                          config = tf.estimator.RunConfig(save_checkpoints_steps = 3000))\n",
    "\n",
    "    elif model_type == 'combined':\n",
    "        model = tf.estimator.DNNLinearCombinedClassifier(model_dir=model_dir,\n",
    "                                                         linear_feature_columns=wide_columns,\n",
    "                                                         dnn_feature_columns=deep_columns,\n",
    "                                                         dnn_hidden_units=[1024, 512, 256],\n",
    "                                                        config = tf.estimator.RunConfig(save_checkpoints_steps = 3000))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create an input function that takes the dataset and serves it to the model in minibatches\n",
    "\n",
    "def input_fn(X, y, mode, batch_size):\n",
    "    if mode == 'train':\n",
    "        return tf.estimator.inputs.numpy_input_fn(x={k: X[k].values for k in FEATURES},\n",
    "                                                    y=y.values,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    num_epochs=None,\n",
    "                                                    shuffle=True,\n",
    "                                                    num_threads=8)\n",
    "\n",
    "    elif mode == 'eval':\n",
    "        return tf.estimator.inputs.numpy_input_fn(x={k: X[k].values for k in FEATURES},\n",
    "                                                    y=y.values,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    num_epochs=None,\n",
    "                                                    shuffle=False,\n",
    "                                                    num_threads=1)\n",
    "    \n",
    "    elif mode == 'predict':\n",
    "        return tf.estimator.inputs.numpy_input_fn(x={k: X[k].values for k in FEATURES},\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    num_epochs=1,\n",
    "                                                    shuffle=False,\n",
    "                                                    num_threads=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model_dir, model_type, train_steps, X_train, y_train, X_test, y_test, batch_size):\n",
    "\n",
    "#Create a temporary directory to store the model if no model directory argument is given\n",
    "\n",
    "    model_dir = tempfile.mkdtemp() if not model_dir else model_dir\n",
    "\n",
    "    print('build_estimator')\n",
    "    model = build_estimator(model_dir, model_type)\n",
    "\n",
    "    print('train start')\n",
    "\n",
    "#Wrap the estimator in an experiment so that metrics are calculated on the training set during training\n",
    "\n",
    "    experiment = tf.contrib.learn.Experiment(estimator=model,\n",
    "                                             train_input_fn=input_fn(X_train,\n",
    "                                                                     y_train,\n",
    "                                                                     mode='train',\n",
    "                                                                     batch_size=batch_size),\n",
    "                                            eval_input_fn=input_fn(X_train,\n",
    "                                                                   y_train,\n",
    "                                                                   mode='eval',\n",
    "                                                                   batch_size=batch_size),\n",
    "                                            train_steps=train_steps,\n",
    "                                            min_eval_frequency=1000)\n",
    "    \n",
    "    experiment.train_and_evaluate()\n",
    "    \n",
    "#Evaluate the trained model on a separate validation set in n/batch_size steps\n",
    "\n",
    "    model.evaluate(input_fn=input_fn(X_test, y_test, mode='eval', batch_size=batch_size))\n",
    "\n",
    "    print('end!')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_estimator\n",
      "INFO:tensorflow:Using config: {'_model_dir': './test-dw-model/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 3000, '_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f288d81ac18>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "train start\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ./test-dw-model/model.ckpt-0\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./test-dw-model/model.ckpt.\n",
      "INFO:tensorflow:loss = 6901.68, step = 1\n",
      "INFO:tensorflow:global_step/sec: 8.96154\n",
      "INFO:tensorflow:loss = 649.345, step = 101 (11.160 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ac703f100329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                          \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                          batch_size=100)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-ccbfbb3ed0db>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_dir, model_type, train_steps, X_train, y_train, X_test, y_test, batch_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m                                             min_eval_frequency=1000)\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#Evaluate the trained model on a separate validation set in n/batch_size steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m                   hooks=self._eval_hooks)\n\u001b[1;32m    624\u001b[0m           ]\n\u001b[0;32m--> 625\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;31m# If the checkpoint_and_export flag and appropriate estimator configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, delay_secs)\u001b[0m\n\u001b[1;32m    365\u001b[0m     return self._call_train(input_fn=self._train_input_fn,\n\u001b[1;32m    366\u001b[0m                             \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                             hooks=self._train_monitors + extra_hooks)\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\u001b[0m in \u001b[0;36m_call_train\u001b[0;34m(self, _sentinel, input_fn, steps, hooks, max_steps)\u001b[0m\n\u001b[1;32m    805\u001b[0m                                    \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                                    \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m                                    hooks=hooks)\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       return self._estimator.fit(input_fn=input_fn,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    519\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    890\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    893\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "deep_model = train_model(model_dir='./test-dw-model/', model_type='combined', train_steps=250000,\n",
    "                         X_train=train_dw, y_train=y_train,\n",
    "                         X_test=val_dw, y_test=y_val,\n",
    "                         batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Make predictions on the test set and write them to a csv file\n",
    "\n",
    "predictions = deep_model.predict(input_fn=input_fn(test_set, None, mode='predict',\n",
    "                                                   batch_size=10000))\n",
    "\n",
    "submission = list()\n",
    "\n",
    "for row in predictions:\n",
    "    submission.append(row['probabilities'][1])\n",
    "    \n",
    "ids = pd.read_csv(\"../../KKBox/input/test.csv\")['id'].values\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "result_df['id'] = ids\n",
    "result_df['target'] = np.array(submission)\n",
    "result_df.to_csv('dw.csv.gz', compression = 'gzip',\n",
    "                 index=False, float_format = '%.5f')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
