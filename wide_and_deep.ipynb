{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Load data and reduce the number of features for the baseline models\n",
    "\n",
    "train = pd.read_csv('../../wl2522/project/dataset/train.csv', dtype={'source_system_tab': str})\n",
    "test = pd.read_csv('../../wl2522/project/dataset/test.csv', dtype={'source_system_tab': str })\n",
    "members = pd.read_csv('../../wl2522/project/dataset/members.csv', dtype={'msno': str, 'city': str, 'registered_via': str})\n",
    "songs = pd.read_csv('../../wl2522/project/dataset/songs.csv', dtype={'genre_ids': str, 'language': str, 'song_length': int})\n",
    "\n",
    "#Infer a missing value based on other features\n",
    "\n",
    "songs.loc[605127, 'language'] = '31.0'\n",
    "\n",
    "#Impute missing values\n",
    "\n",
    "train.fillna(value='unknown', axis=1, inplace=True)\n",
    "test.fillna(value='unknown', axis=1, inplace=True)\n",
    "members.fillna(value='unknown', axis=1, inplace=True)\n",
    "songs.fillna(value='unknown', axis=1, inplace=True)\n",
    "\n",
    "#Convert feature columns related to member registration to datetime format\n",
    "\n",
    "members['expiration_date'] = pd.to_datetime(members['expiration_date'], format='%Y%m%d')\n",
    "members['registration_init_time'] = pd.to_datetime(members['registration_init_time'], format='%Y%m%d')\n",
    "\n",
    "#Create a feature indicating the number of days a member was registered\n",
    "\n",
    "members['reg_duration'] = (members['expiration_date'] - members['registration_init_time']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create lists of genre ids in the genre id column\n",
    "\n",
    "#genres = songs['genre_ids'].str.split('|')\n",
    "\n",
    "#Create a dataframe that stores genre IDs across multiple columns (one genre per column)\n",
    "\n",
    "#genres = genres.apply(pd.Series).add_prefix('genre_')\n",
    "#genres.to_csv('dataset/genres.csv', index=False)\n",
    "\n",
    "genres = pd.read_csv('../../wl2522/project/dataset/genres.csv', dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Merge the training and test data with song and member data\n",
    "\n",
    "train_set = train.merge(songs, on='song_id')\n",
    "train_set = train_set.merge(members, on='msno')\n",
    "test_set = test.merge(songs, on='song_id', how='left')\n",
    "test_set = test_set.merge(members, on='msno', how='left')\n",
    "\n",
    "#Separate the submission ids from the test set\n",
    "\n",
    "ids = test_set['id']\n",
    "test_set.drop('id', axis=1, inplace=True)\n",
    "\n",
    "#Impute missing values for specific features in the merged training and test sets with an integer value \n",
    "\n",
    "msno_vocabs = train_set['msno'].unique()\n",
    "msno_map = { msno_vocabs[i]:i+1 for i in range(len(msno_vocabs))}\n",
    "train_set['msno'] = train_set['msno'].map(msno_map).fillna(-99).astype(int)\n",
    "test_set['msno'] = test_set['msno'].map(msno_map).fillna(-99).astype(int)\n",
    "\n",
    "song_vocabs = train_set['song_id'].unique()\n",
    "song_map = { song_vocabs[i]:i+1 for i in range(len(song_vocabs))}\n",
    "train_set['song_id'] = train_set['song_id'].map(song_map).fillna(-99).astype(int)\n",
    "test_set['song_id'] = test_set['song_id'].map(song_map).fillna(-99).astype(int)\n",
    "\n",
    "tab_vocabs = train_set['source_system_tab'].unique()\n",
    "tab_map = { tab_vocabs[i]:i+1 for i in range(len(tab_vocabs))}\n",
    "train_set['source_system_tab'] = train_set['source_system_tab'].map(tab_map).fillna(-99).astype(int)\n",
    "test_set['source_system_tab'] = test_set['source_system_tab'].map(tab_map).fillna(-99).astype(int)\n",
    "\n",
    "screen_vocabs = train_set['source_screen_name'].unique()\n",
    "screen_map = { screen_vocabs[i]:i+1 for i in range(len(screen_vocabs))}\n",
    "train_set['source_screen_name'] = train_set['source_screen_name'].map(screen_map).fillna(-99).astype(int)\n",
    "test_set['source_screen_name'] = test_set['source_screen_name'].map(screen_map).fillna(-99).astype(int)\n",
    "\n",
    "sctype_vocabs = train_set['source_type'].unique()\n",
    "sctype_map = { sctype_vocabs[i]:i+1 for i in range(len(sctype_vocabs))}\n",
    "train_set['source_type'] = train_set['source_type'].map(sctype_map).fillna(-99).astype(int)\n",
    "test_set['source_type'] = test_set['source_type'].map(sctype_map).fillna(-99).astype(int)\n",
    "\n",
    "gid_vocabs = train_set['genre_ids'].unique()\n",
    "gid_map = { gid_vocabs[i]:i+1 for i in range(len(gid_vocabs))}\n",
    "train_set['genre_ids'] = train_set['genre_ids'].map(gid_map).fillna(-99).astype(int)\n",
    "test_set['genre_ids'] = test_set['genre_ids'].map(gid_map).fillna(-99).astype(int)\n",
    "\n",
    "gender_vocabs = train_set['gender'].unique()\n",
    "gender_map = { gender_vocabs[i]:i+1 for i in range(len(gender_vocabs))}\n",
    "train_set['gender'] = train_set['gender'].map(gender_map).fillna(-99).astype(int)\n",
    "test_set['gender'] = test_set['gender'].map(gender_map).fillna(-99).astype(int)\n",
    "\n",
    "city_vocabs = train_set['city'].unique()\n",
    "city_map = { city_vocabs[i]:i+1 for i in range(len(city_vocabs))}\n",
    "train_set['city'] = train_set['city'].map(city_map).fillna(-99).astype(int)\n",
    "test_set['city'] = test_set['city'].map(city_map).fillna(-99).astype(int)\n",
    "\n",
    "lan_vocabs = train_set['language'].unique()\n",
    "lan_map = { lan_vocabs[i]:i+1 for i in range(len(lan_vocabs))}\n",
    "train_set['language'] = train_set['language'].map(lan_map).fillna(-99).astype(int)\n",
    "test_set['language'] = test_set['language'].map(lan_map).fillna(-99).astype(int)\n",
    "\n",
    "#Impute missing values for the remaining features with a string\n",
    "\n",
    "train_set.fillna(value='unknown', axis=1, inplace=True)\n",
    "test_set.fillna(value='unknown', axis=1, inplace=True)\n",
    "\n",
    "#Impute missing song lengths with an integer value to avoid errors due to conflicting data types\n",
    "\n",
    "test_set['song_length'] = test_set['song_length'].replace('unknown', -99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#load the pretrained GBDT model in order to use its predictions as a feature in the final model\n",
    "\n",
    "train_wide = pd.read_csv('tmp/train.csv', index_col=0)\n",
    "test_wide = pd.read_csv('tmp/test.csv', index_col=0)\n",
    "train_wide.drop('target', axis=1, inplace=True)\n",
    "\n",
    "feature_trans = joblib.load('./tmp/lgb.pkl')\n",
    "tr_features = feature_trans.predict(train_wide.values, pred_leaf=True)\n",
    "tes_features = feature_trans.predict(test_wide.values, pred_leaf=True)\n",
    "\n",
    "width = tr_features.shape[1]\n",
    "trans_feature = ['trans_feature' + str(i) for i in range(width)]\n",
    "train_trans = pd.DataFrame()\n",
    "test_trans = pd.DataFrame()\n",
    "\n",
    "for i in range(len(trans_feature)):\n",
    "    train_trans[trans_feature[i]] = tr_features[:, i]\n",
    "    test_trans[trans_feature[i]] = tes_features[:, i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Concatenate the original dataset with the GBDT model predictions\n",
    "\n",
    "train_dw = pd.concat([train_set, train_trans], axis=1)\n",
    "test_dw = pd.concat([test_set, test_trans], axis=1)\n",
    "\n",
    "#Impute values for missing song data and gender\n",
    "\n",
    "train_dw['song_length'] = pd.to_numeric(train_dw['song_length'],\n",
    "                                        downcast='integer', errors='coerce').fillna(0)\n",
    "test_dw['song_length'] = pd.to_numeric(test_dw['song_length'],\n",
    "                                       downcast='integer', errors='coerce').fillna(0)\n",
    "\n",
    "test_dw['gender'] = test_dw['gender'].fillna(-99)\n",
    "train_dw['gender'] = train_dw['gender'].fillna(-99)\n",
    "test_dw['gender'] = test_dw['gender'].astype(int)\n",
    "train_dw['gender'] = train_dw['gender'].astype(int)\n",
    "\n",
    "#Convert the song length and gender values to integer to avoid data type errors\n",
    "\n",
    "train_dw['song_length'] = train_dw['song_length'].astype('int32')\n",
    "test_dw['song_length'] = test_dw['song_length'].astype('int32')\n",
    "\n",
    "train_dw.to_csv('tmp/train_dw.csv')\n",
    "test_dw.to_csv('tmp/test_dw.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert feature column values back to integers for compatibility with the feature column API\n",
    "\n",
    "for column in ['gender', 'city', 'language',\n",
    "               'source_system_tab', 'source_screen_name', 'source_type', 'genre_ids']:\n",
    "    test_dw[column] = test_dw[column].fillna(-99)\n",
    "    train_dw[column] = train_dw[column].fillna(-99)\n",
    "    test_dw[column] = test_dw[column].astype(int)\n",
    "    train_dw[column] = train_dw[column].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Shuffle the data and split off 20% of the training set for use as a validation set\n",
    "\n",
    "split_ratio = 0.8\n",
    "\n",
    "train_dw = train_dw.sample(frac=1, random_state=6)\n",
    "val_dw = train_dw[int(split_ratio*train_dw.shape[0]):]\n",
    "train_dw = train_dw[:int(split_ratio*train_dw.shape[0])]\n",
    "\n",
    "#Separate the labels from the training and validation sets\n",
    "\n",
    "y_train = train_dw['target']\n",
    "train_dw.drop('target', axis=1, inplace=True)\n",
    "\n",
    "y_val = val_dw['target']\n",
    "val_dw.drop('target', axis=1, inplace=True)\n",
    "\n",
    "train_set = train_dw\n",
    "test_set = test_dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Designate the target feature name and the features to be used in the dataset\n",
    "\n",
    "FEATURES = ['msno', 'gender', 'city', 'bd',\n",
    "            'song_id', 'language', 'genre_ids', 'composer', 'lyricist',\n",
    "            'source_system_tab', 'source_screen_name', 'source_type', 'reg_duration']\n",
    "\n",
    "#Include the GBDT predictions as features in the final model (one for each tree in the GBDT model, 32 leaves per tree)\n",
    "\n",
    "width = 60\n",
    "trans_feature = ['trans_feature' + str(i) for i in range(width)]\n",
    "FEATURES.extend(trans_feature)\n",
    "\n",
    "#Use the feature_column module to input each feature column into the model\n",
    "\n",
    "target = tf.feature_column.categorical_column_with_identity(key='target', num_buckets=2)\n",
    "\n",
    "length = tf.feature_column.numeric_column(key='song_length',\n",
    "                                          default_value=-1,\n",
    "                                          dtype=tf.int32)\n",
    "\n",
    "duration = tf.feature_column.numeric_column(key='reg_duration',\n",
    "                                            default_value=-1,\n",
    "                                            dtype=tf.int32)\n",
    "\n",
    "gender = tf.feature_column.categorical_column_with_vocabulary_list(key='gender',\n",
    "                                                                   vocabulary_list=train_dw['gender'].unique(),\n",
    "                                                                   dtype=tf.int32,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "city = tf.feature_column.categorical_column_with_vocabulary_list(key='city',\n",
    "                                                          vocabulary_list=train_dw['city'].unique(),\n",
    "                                                          dtype=tf.int32,\n",
    "                                                          default_value=-99)\n",
    "\n",
    "language = tf.feature_column.categorical_column_with_vocabulary_list(key='language',\n",
    "                                                                     vocabulary_list=train_dw['language'].unique(),\n",
    "                                                                     dtype=tf.int32,\n",
    "                                                                     default_value=-99)\n",
    "\n",
    "composer = tf.feature_column.categorical_column_with_vocabulary_list(key='composer',\n",
    "                                                                     vocabulary_list=songs['composer'].unique(),\n",
    "                                                                     dtype=tf.string,\n",
    "                                                                     default_value=-99)\n",
    "\n",
    "lyricist = tf.feature_column.categorical_column_with_vocabulary_list(key='lyricist',\n",
    "                                                                     vocabulary_list=songs['lyricist'].unique(),\n",
    "                                                                     dtype=tf.string,\n",
    "                                                                     default_value=-99)\n",
    "\n",
    "tab = tf.feature_column.categorical_column_with_vocabulary_list(key='source_system_tab',\n",
    "                                                                vocabulary_list=train_set['source_system_tab'].unique(),\n",
    "                                                                dtype=tf.int32,\n",
    "                                                                default_value=-99)\n",
    "\n",
    "screen = tf.feature_column.categorical_column_with_vocabulary_list(key='source_screen_name',\n",
    "                                                                   vocabulary_list=train_set['source_screen_name'].unique(),\n",
    "                                                                   dtype=tf.int32,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "source = tf.feature_column.categorical_column_with_vocabulary_list(key='source_type',\n",
    "                                                                   vocabulary_list=train_set['source_type'].unique(),\n",
    "                                                                   dtype=tf.int32,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "#Bucket categorical features with many unique categories using a hash table with a size of approximately (n/0.8)*2\n",
    "\n",
    "msno = tf.feature_column.categorical_column_with_hash_bucket(key='msno',\n",
    "                                                               hash_bucket_size=90000,\n",
    "                                                               dtype=tf.int32)\n",
    "\n",
    "song_id = tf.feature_column.categorical_column_with_hash_bucket(key='song_id',\n",
    "                                                             hash_bucket_size=6000000,\n",
    "                                                             dtype=tf.int32)\n",
    "\n",
    "hashed_genre = tf.feature_column.categorical_column_with_hash_bucket(key='genre_ids',\n",
    "                                                                     hash_bucket_size=3000,\n",
    "                                                                     dtype=tf.int32)\n",
    "\n",
    "#Perform one hot encoding on categorical features with few unique values\n",
    "\n",
    "indicator_gender = tf.feature_column.indicator_column(gender)\n",
    "indicator_city = tf.feature_column.indicator_column(city)\n",
    "indicator_language = tf.feature_column.indicator_column(language)\n",
    "indicator_tab = tf.feature_column.indicator_column(tab)\n",
    "indicator_screen = tf.feature_column.indicator_column(screen)\n",
    "indicator_source = tf.feature_column.indicator_column(source)\n",
    "\n",
    "#Embed the categorical feature with <100 unique categories into dense vectors with approximately log2(n) dimensions\n",
    "\n",
    "embedded_genre = tf.feature_column.embedding_column(hashed_genre, dimension=10)\n",
    "embedded_song = tf.feature_column.embedding_column(song_id, dimension=22)\n",
    "embedded_msno = tf.feature_column.embedding_column(msno, dimension=15)\n",
    "embedded_composer = tf.feature_column.embedding_column(composer, dimension=18)\n",
    "embedded_lyricist = tf.feature_column.embedding_column(lyricist, dimension=17)\n",
    "\n",
    "#Bucket member age into age ranges, with nonsensical values going into the 0-14 or the >80 buckets\n",
    "\n",
    "age = tf.feature_column.numeric_column(key='bd',\n",
    "                                       default_value=0,\n",
    "                                       dtype=tf.int32)\n",
    "\n",
    "age_bucket = tf.feature_column.bucketized_column(age, boundaries=[0, 14, 20, 30, 40, 50, 80])\n",
    "\n",
    "#Assign features to be used in either the wide or the deep model\n",
    "\n",
    "wide_columns =  [tf.feature_column.categorical_column_with_vocabulary_list(key='trans_feature' + str(i),\n",
    "                                                                   vocabulary_list=list(range(2**5)),\n",
    "                                                                   dtype=tf.int32,\n",
    "                                                                   default_value=-99) for i in range(60)]\n",
    "\n",
    "deep_columns = [indicator_gender, indicator_city, indicator_language,\n",
    "                indicator_tab, indicator_screen, indicator_source,\n",
    "                embedded_genre, embedded_msno, embedded_song,\n",
    "                embedded_composer, embedded_lyricist,\n",
    "                duration, age_bucket]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create an instance of the TensorFlow estimator for the specified model\n",
    "\n",
    "def build_estimator(model_dir, model_type):\n",
    "    if model_type == 'wide':\n",
    "        model = tf.estimator.LinearClassifier(model_dir=model_dir,\n",
    "                                              feature_columns=wide_columns)\n",
    "\n",
    "    elif model_type == 'deep':\n",
    "        model = tf.estimator.DNNClassifier(model_dir=model_dir,\n",
    "                                           feature_columns=deep_columns,\n",
    "                                           hidden_units=[1024, 512, 256],\n",
    "                                           optimizer=tf.train.AdamOptimizer(learning_rate=0.001,\n",
    "                                                                            name='Adam'),\n",
    "                                          config = tf.estimator.RunConfig(save_checkpoints_steps = 3000))\n",
    "\n",
    "    elif model_type == 'combined':\n",
    "        model = tf.estimator.DNNLinearCombinedClassifier(model_dir=model_dir,\n",
    "                                                         linear_feature_columns=wide_columns,\n",
    "                                                         dnn_feature_columns=deep_columns,\n",
    "                                                         dnn_hidden_units=[1024, 512, 256],\n",
    "                                                        config = tf.estimator.RunConfig(save_checkpoints_steps = 3000))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create an input function that takes the dataset and serves it to the model in minibatches\n",
    "\n",
    "def input_fn(X, y, mode, batch_size):\n",
    "    X.fillna(value='unknown', axis=1, inplace=True)    \n",
    "\n",
    "    if mode == 'train':\n",
    "        return tf.estimator.inputs.numpy_input_fn(x={k: X[k].values for k in FEATURES},\n",
    "                                                    y=y.values,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    num_epochs=None,\n",
    "                                                    shuffle=True,\n",
    "                                                    num_threads=8)\n",
    "\n",
    "    elif mode == 'eval':\n",
    "        return tf.estimator.inputs.numpy_input_fn(x={k: X[k].values for k in FEATURES},\n",
    "                                                    y=y.values,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    num_epochs=None,\n",
    "                                                    shuffle=False,\n",
    "                                                    num_threads=1)\n",
    "    \n",
    "    elif mode == 'predict':\n",
    "        return tf.estimator.inputs.numpy_input_fn(x={k: X[k].values for k in FEATURES},\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    num_epochs=1,\n",
    "                                                    shuffle=False,\n",
    "                                                    num_threads=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model_dir, model_type, train_steps, X_train, y_train, X_test, y_test, batch_size):\n",
    "\n",
    "#Create a temporary directory to store the model if no model directory argument is given\n",
    "\n",
    "    model_dir = tempfile.mkdtemp() if not model_dir else model_dir\n",
    "\n",
    "    print('build_estimator')\n",
    "    model = build_estimator(model_dir, model_type)\n",
    "\n",
    "    print('train start')\n",
    "\n",
    "#Wrap the estimator in an experiment so that metrics are calculated on the training set during training\n",
    "\n",
    "    experiment = tf.contrib.learn.Experiment(estimator=model,\n",
    "                                             train_input_fn=input_fn(X_train,\n",
    "                                                                     y_train,\n",
    "                                                                     mode='train',\n",
    "                                                                     batch_size=batch_size),\n",
    "                                            eval_input_fn=input_fn(X_train,\n",
    "                                                                   y_train,\n",
    "                                                                   mode='eval',\n",
    "                                                                   batch_size=batch_size),\n",
    "                                            train_steps=train_steps,\n",
    "                                            min_eval_frequency=1000)\n",
    "    \n",
    "    experiment.train_and_evaluate()\n",
    "    \n",
    "#Evaluate the trained model on a separate validation set in n/batch_size steps\n",
    "\n",
    "    model.evaluate(input_fn=input_fn(X_test, y_test, mode='eval', batch_size=batch_size))\n",
    "\n",
    "    print('end!')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "deep_model = train_model(model_dir='./test-dw-model/', model_type='combined', train_steps=250000,\n",
    "                         X_train=train_dw, y_train=y_train,\n",
    "                         X_test=val_dw, y_test=y_val,\n",
    "                         batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Make predictions on the test set and write them to a csv file\n",
    "\n",
    "predictions = deep_model.predict(input_fn=input_fn(test_set, None, mode='predict',\n",
    "                                                   batch_size=10000))\n",
    "\n",
    "submission = list()\n",
    "\n",
    "for row in predictions:\n",
    "    submission.append(row['probabilities'][1])\n",
    "    \n",
    "ids = pd.read_csv(\"../../KKBox/input/test.csv\")['id'].values\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "result_df['id'] = ids\n",
    "result_df['target'] = np.array(submission)\n",
    "result_df.to_csv('dw.csv.gz', compression = 'gzip',\n",
    "                 index=False, float_format = '%.5f')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
