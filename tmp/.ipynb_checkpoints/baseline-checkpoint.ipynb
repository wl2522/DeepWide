{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T08:30:06.827077Z",
     "start_time": "2017-11-21T08:30:00.586530Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T08:30:41.256601Z",
     "start_time": "2017-11-21T08:30:07.827727Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Load data and reduce the number of features for the baseline models\n",
    "\n",
    "train = pd.read_csv('dataset/train.csv', dtype={'source_system_tab': str})\n",
    "test = pd.read_csv('dataset/test.csv', dtype={'source_system_tab': str })\n",
    "members = pd.read_csv('dataset/members.csv', dtype={'msno': str, 'city': str, 'registered_via': str})\n",
    "songs = pd.read_csv('dataset/songs.csv', dtype={'genre_ids': str, 'language': str, 'song_length': int})\n",
    "\n",
    "songs.drop(['composer', 'lyricist'], axis=1, inplace=True)\n",
    "\n",
    "#Infer a missing value based on other features\n",
    "\n",
    "songs.loc[605127, 'language'] = '31.0'\n",
    "\n",
    "#Impute missing values\n",
    "\n",
    "train.fillna(value='unknown', axis=1, inplace=True)\n",
    "test.fillna(value='unknown', axis=1, inplace=True)\n",
    "members.fillna(value='unknown', axis=1, inplace=True)\n",
    "songs.fillna(value='unknown', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-19T06:05:41.816928Z",
     "start_time": "2017-11-19T06:05:41.813924Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#members['registration_init_time'].sort_values()\n",
    "#members['expiration_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T08:38:52.141228Z",
     "start_time": "2017-11-21T08:30:43.476687Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Create lists of genre ids in the genre id column\n",
    "\n",
    "genres = songs['genre_ids'].str.split('|')\n",
    "\n",
    "#Create a dataframe that stores genre IDs across multiple columns (one genre per column)\n",
    "\n",
    "genres = genres.apply(pd.Series).add_prefix('genre_')\n",
    "genres.to_csv('dataset/genres.csv', index=False)\n",
    "\n",
    "genres = pd.read_csv('dataset/genres.csv', dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T08:39:27.947215Z",
     "start_time": "2017-11-21T08:38:52.141228Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Merge the training and test data with song and member data\n",
    "\n",
    "train_set = train.merge(songs, on='song_id')\n",
    "train_set = train_set.merge(members, on='msno')\n",
    "test_set = test.merge(songs, on='song_id', how='left')\n",
    "test_set = test_set.merge(members, on='msno', how='left')\n",
    "\n",
    "#Separate the submission ids from the test set\n",
    "\n",
    "ids = test_set['id']\n",
    "test_set.drop('id', axis=1, inplace=True)\n",
    "\n",
    "#Impute missing values in merged training and test sets\n",
    "\n",
    "train_set.fillna(value='unknown', axis=1, inplace=True)\n",
    "test_set.fillna(value='unknown', axis=1, inplace=True)\n",
    "\n",
    "#Impute missing song lengths with an integer value to avoid errors due to conflicting data types\n",
    "\n",
    "test_set['song_length'] = test_set['song_length'].replace('unknown', -99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T08:40:14.691359Z",
     "start_time": "2017-11-21T08:39:27.947215Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Shuffle the data and split off 20% of the training set for use as a validation set\n",
    "\n",
    "split_ratio = 0.8\n",
    "\n",
    "train_set = train_set.sample(frac=1, random_state=6)\n",
    "val_set = train_set[int(split_ratio*train_set.shape[0]):]\n",
    "train_set = train_set[:int(split_ratio*train_set.shape[0])]\n",
    "\n",
    "#Separate the labels from the training and validation sets\n",
    "\n",
    "y_train = train_set['target']\n",
    "train_set.drop('target', axis=1, inplace=True)\n",
    "\n",
    "y_val = val_set['target']\n",
    "val_set.drop('target', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T08:40:16.463412Z",
     "start_time": "2017-11-21T08:40:14.691359Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Designate the target feature name and the features to be used in the dataset\n",
    "\n",
    "FEATURES = ['msno', 'gender', 'city', 'bd', 'registered_via',\n",
    "            'song_id', 'artist_name', 'song_length', 'language', 'genre_ids',\n",
    "            'source_system_tab', 'source_screen_name', 'source_type']\n",
    "\n",
    "LABEL = 'target'\n",
    "\n",
    "#Use the feature_column module to input each feature column into the model\n",
    "\n",
    "target = tf.feature_column.categorical_column_with_identity(key='target', num_buckets=2)\n",
    "\n",
    "registered = tf.feature_column.categorical_column_with_vocabulary_list(key='registered_via',\n",
    "                                                                       vocabulary_list=['7', '4', '9', '3', '13', '16'],\n",
    "                                                                       dtype=tf.string,\n",
    "                                                                       default_value=-99)\n",
    "\n",
    "gender = tf.feature_column.categorical_column_with_vocabulary_list(key='gender',\n",
    "                                                                   vocabulary_list=('female', 'male', 'unknown'),\n",
    "                                                                   dtype=tf.string,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "city = tf.feature_column.categorical_column_with_vocabulary_list(key='city',\n",
    "                                                          vocabulary_list=members['city'].unique(),\n",
    "                                                          dtype=tf.string,\n",
    "                                                          default_value=-99)\n",
    "\n",
    "language = tf.feature_column.categorical_column_with_vocabulary_list(key='language',\n",
    "                                                                     vocabulary_list=songs['language'].unique(),\n",
    "                                                                     dtype=tf.string,\n",
    "                                                                     default_value=-99)\n",
    "\n",
    "artist = tf.feature_column.categorical_column_with_vocabulary_list(key='artist_name',\n",
    "                                                                   vocabulary_list=songs['artist_name'].unique(),\n",
    "                                                                   dtype=tf.string,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "tab = tf.feature_column.categorical_column_with_vocabulary_list(key='source_system_tab',\n",
    "                                                                vocabulary_list=train['source_system_tab'].unique(),\n",
    "                                                                dtype=tf.string,\n",
    "                                                                default_value=-99)\n",
    "\n",
    "screen = tf.feature_column.categorical_column_with_vocabulary_list(key='source_screen_name',\n",
    "                                                                   vocabulary_list=train['source_screen_name'].unique(),\n",
    "                                                                   dtype=tf.string,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "source = tf.feature_column.categorical_column_with_vocabulary_list(key='source_type',\n",
    "                                                                   vocabulary_list=train['source_type'].unique(),\n",
    "                                                                   dtype=tf.string,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "length = tf.feature_column.numeric_column(key='song_length',\n",
    "                                          default_value=-1,\n",
    "                                          dtype=tf.int32)\n",
    "\n",
    "#Bucket categorical features with many unique categories using a hash table with a size of approximately (n/0.8)*2\n",
    "\n",
    "msno = tf.feature_column.categorical_column_with_hash_bucket(key='msno',\n",
    "                                                               hash_bucket_size=90000,\n",
    "                                                               dtype=tf.string)\n",
    "\n",
    "song_id = tf.feature_column.categorical_column_with_hash_bucket(key='song_id',\n",
    "                                                             hash_bucket_size=6000000,\n",
    "                                                             dtype=tf.string)\n",
    "\n",
    "genre = tf.feature_column.categorical_column_with_vocabulary_list(key='genre_ids',\n",
    "                                                                  vocabulary_list=genres['genre_0'].unique(),\n",
    "                                                                  dtype=tf.string,\n",
    "                                                                  default_value=-99)\n",
    "\n",
    "hashed_genre = tf.feature_column.categorical_column_with_hash_bucket(key='genre_ids',\n",
    "                                                                     hash_bucket_size=3000,\n",
    "                                                                     dtype=tf.string)\n",
    "\n",
    "#Perform one hot encoding on categorical features with few unique values\n",
    "\n",
    "indicator_registered = tf.feature_column.indicator_column(registered)\n",
    "indicator_gender = tf.feature_column.indicator_column(gender)\n",
    "indicator_city = tf.feature_column.indicator_column(city)\n",
    "indicator_genre = tf.feature_column.indicator_column(genre)\n",
    "indicator_language = tf.feature_column.indicator_column(language)\n",
    "indicator_tab = tf.feature_column.indicator_column(tab)\n",
    "indicator_screen = tf.feature_column.indicator_column(screen)\n",
    "indicator_source = tf.feature_column.indicator_column(source)\n",
    "\n",
    "#Embed the categorical feature with <100 unique categories into dense vectors with approximately log2(n) dimensions\n",
    "\n",
    "embedded_genre = tf.feature_column.embedding_column(genre, dimension=10)\n",
    "embedded_song = tf.feature_column.embedding_column(song_id, dimension=22)\n",
    "embedded_msno = tf.feature_column.embedding_column(msno, dimension=15)\n",
    "embedded_artist = tf.feature_column.embedding_column(artist, dimension=18)\n",
    "\n",
    "#Bucket member age into age ranges, with nonsensical values going into the 0-14 or the >80 buckets\n",
    "\n",
    "age = tf.feature_column.numeric_column(key='bd',\n",
    "                                       default_value=0,\n",
    "                                       dtype=tf.int32)\n",
    "\n",
    "age_bucket = tf.feature_column.bucketized_column(age, boundaries=[0, 14, 20, 30, 40, 50, 80])\n",
    "\n",
    "#Assign features to be used in either the wide or the deep model (or both)\n",
    "\n",
    "wide_columns = []\n",
    "cross_columns = []\n",
    "deep_columns = [\n",
    "                indicator_gender, indicator_city, indicator_language, indicator_tab,\n",
    "                indicator_screen, indicator_source, indicator_registered,\n",
    "                #embedded_msno, embedded_song, embedded_artist, embedded_genre,\n",
    "                #length, age_bucket\n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T08:40:16.480768Z",
     "start_time": "2017-11-21T08:40:16.463412Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, model_type):\n",
    "    if model_type == 'wide':\n",
    "        model = tf.estimator.LinearClassifier(model_dir=model_dir,\n",
    "                                              feature_columns=wide_columns + cross_columns)\n",
    "\n",
    "    elif model_type == 'deep':\n",
    "        model = tf.estimator.DNNClassifier(model_dir=model_dir,\n",
    "                                           feature_columns=deep_columns,\n",
    "                                           hidden_units=[1024, 512, 256],\n",
    "                                           optimizer=tf.train.AdamOptimizer(learning_rate=0.001,\n",
    "                                                                            name='Adam'))\n",
    "\n",
    "    elif model_type == 'combined':\n",
    "        model = tf.estimator.DNNLinearCombinedClassifier(model_dir=model_dir,\n",
    "                                                         linear_feature_columns=cross_columns,\n",
    "                                                         dnn_feature_columns=deep_columns,\n",
    "                                                         dnn_hidden_units=[100, 50])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T08:40:16.502790Z",
     "start_time": "2017-11-21T08:40:16.481769Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def input_fn(X, y, mode, batch_size):\n",
    "    print(X.shape)\n",
    "    X.fillna(value='unknown', axis=1, inplace=True)    \n",
    "\n",
    "    if mode == 'train':\n",
    "        return tf.estimator.inputs.pandas_input_fn(x=pd.DataFrame({k: X[k].values for k in FEATURES}),\n",
    "                                                   y=pd.Series(y.values),\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_epochs=None,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_threads=8,\n",
    "                                                   target_column='target')\n",
    "    \n",
    "    elif mode == 'eval':\n",
    "        return tf.estimator.inputs.pandas_input_fn(x = pd.DataFrame({k: X[k].values for k in FEATURES}),\n",
    "                                                   y = pd.Series(y.values),\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_epochs=1,\n",
    "                                                   shuffle=False,\n",
    "                                                   num_threads=1,\n",
    "                                                   target_column='target')\n",
    "    \n",
    "    elif mode == 'predict':\n",
    "        return tf.estimator.inputs.pandas_input_fn(x=pd.DataFrame({k: X[k].values for k in FEATURES}),\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_epochs=1,\n",
    "                                                   shuffle=False,\n",
    "                                                   num_threads=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T08:40:16.527813Z",
     "start_time": "2017-11-21T08:40:16.503791Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model_dir, model_type, train_steps, X_train, y_train, X_test, y_test, batch_size):\n",
    "\n",
    "#Create a temporary directory to store the model if no model directory argument is given\n",
    "\n",
    "    model_dir = tempfile.mkdtemp() if not model_dir else model_dir\n",
    "    \n",
    "    print('build_estimator')\n",
    "    model = build_estimator(model_dir, model_type)\n",
    "    \n",
    "    print('train start')\n",
    "    model.train(input_fn=input_fn(X_train, y_train, mode='train', batch_size=batch_size),\n",
    "                max_steps=train_steps)\n",
    "    \n",
    "#Evaluate the trained model on a separate validation set in n/batch_size steps\n",
    "    \n",
    "    model.evaluate(input_fn=input_fn(X_test, y_test, mode='eval', batch_size=batch_size),\n",
    "                        steps=(X_test.shape[0]//batch_size + 1))\n",
    "\n",
    "    print('end!')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T09:56:10.044495Z",
     "start_time": "2017-11-21T08:40:16.528814Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deep_model = train_model(model_dir='model/', model_type='deep', train_steps=600000,\n",
    "                         X_train=train_set, y_train=y_train,\n",
    "                         X_test=val_set, y_test=y_val,\n",
    "                         batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T09:56:52.357829Z",
     "start_time": "2017-11-21T09:56:10.046496Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "results = deep_model.evaluate(input_fn=input_fn(train_set, y_train, mode='eval', batch_size=1000),\n",
    "                              steps=1476)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-21T09:57:38.223184Z",
     "start_time": "2017-11-21T09:56:52.360701Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = deep_model.predict(input_fn=input_fn(test_set, None, mode='predict',\n",
    "                                                   batch_size=10000))\n",
    "\n",
    "submission = list()\n",
    "\n",
    "for row in predictions:\n",
    "    submission.append(row['probabilities'][1])\n",
    "\n",
    "pd.DataFrame(data={'id': ids,\n",
    "                   'target': np.array(submission)}).to_csv('submissions/benchmark_deep.csv',\n",
    "                                                           header=['id', 'target'],\n",
    "                                                           index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
