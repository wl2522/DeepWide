{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../../wl2522/project/dataset/train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-690eb9cfa472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load data and reduce the number of features for the baseline models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../wl2522/project/dataset/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'source_system_tab'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../wl2522/project/dataset/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'source_system_tab'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmembers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../wl2522/project/dataset/members.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'msno'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'city'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'registered_via'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    703\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../../wl2522/project/dataset/train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#Load data and reduce the number of features for the baseline models\n",
    "\n",
    "train = pd.read_csv('../../wl2522/project/dataset/train.csv', dtype={'source_system_tab': str})\n",
    "test = pd.read_csv('../../wl2522/project/dataset/test.csv', dtype={'source_system_tab': str })\n",
    "members = pd.read_csv('../../wl2522/project/dataset/members.csv', dtype={'msno': str, 'city': str, 'registered_via': str})\n",
    "songs = pd.read_csv('../../wl2522/project/dataset/songs.csv', dtype={'genre_ids': str, 'language': str, 'song_length': int})\n",
    "\n",
    "songs.drop(['composer', 'lyricist'], axis=1, inplace=True)\n",
    "\n",
    "#Infer a missing value based on other features\n",
    "\n",
    "songs.loc[605127, 'language'] = '31.0'\n",
    "\n",
    "#Impute missing values\n",
    "\n",
    "train.fillna(value='unknown', axis=1, inplace=True)\n",
    "test.fillna(value='unknown', axis=1, inplace=True)\n",
    "members.fillna(value='unknown', axis=1, inplace=True)\n",
    "songs.fillna(value='unknown', axis=1, inplace=True)\n",
    "\n",
    "#Convert feature columns related to member registration to datetime format\n",
    "\n",
    "members['expiration_date'] = pd.to_datetime(members['expiration_date'], format='%Y%m%d')\n",
    "members['registration_init_time'] = pd.to_datetime(members['registration_init_time'], format='%Y%m%d')\n",
    "\n",
    "#Create a new feature indicating the number of days a member was registered\n",
    "\n",
    "members['reg_duration'] = (members['expiration_date'] - members['registration_init_time']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../../wl2522/project/dataset/genres.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7bf94721dc06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#genres.to_csv('dataset/genres.csv', index=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgenres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../wl2522/project/dataset/genres.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    703\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../../wl2522/project/dataset/genres.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#Create lists of genre ids in the genre id column\n",
    "\n",
    "#genres = songs['genre_ids'].str.split('|')\n",
    "\n",
    "#Create a dataframe that stores genre IDs across multiple columns (one genre per column)\n",
    "\n",
    "#genres = genres.apply(pd.Series).add_prefix('genre_')\n",
    "#genres.to_csv('dataset/genres.csv', index=False)\n",
    "\n",
    "genres = pd.read_csv('../../wl2522/project/dataset/genres.csv', dtype=str)\n",
    "#Merge the training and test data with song and member data\n",
    "\n",
    "train_set = train.merge(songs, on='song_id')\n",
    "train_set = train_set.merge(members, on='msno')\n",
    "test_set = test.merge(songs, on='song_id', how='left')\n",
    "test_set = test_set.merge(members, on='msno', how='left')\n",
    "\n",
    "#Separate the submission ids from the test set\n",
    "\n",
    "ids = test_set['id']\n",
    "test_set.drop('id', axis=1, inplace=True)\n",
    "\n",
    "#Impute missing values in merged training and test sets\n",
    "\n",
    "train_set.fillna(value='unknown', axis=1, inplace=True)\n",
    "test_set.fillna(value='unknown', axis=1, inplace=True)\n",
    "\n",
    "#Impute missing song lengths with an integer value to avoid errors due to conflicting data types\n",
    "\n",
    "test_set['song_length'] = test_set['song_length'].replace('unknown', -99)\n",
    "\n",
    "msno_vocabs = train_set['msno'].unique()\n",
    "msno_map = { msno_vocabs[i]:i+1 for i in range(len(msno_vocabs))}\n",
    "train_set['msno'] = train_set['msno'].map(msno_map).fillna(3).astype(int)\n",
    "test_set['msno'] = test_set['msno'].map(msno_map).fillna(3).astype(int)\n",
    "\n",
    "song_vocabs = train_set['song_id'].unique()\n",
    "song_map = { song_vocabs[i]:i+1 for i in range(len(song_vocabs))}\n",
    "train_set['song_id'] = train_set['song_id'].map(song_map).fillna(3).astype(int)\n",
    "test_set['song_id'] = test_set['song_id'].map(song_map).fillna(3).astype(int)\n",
    "\n",
    "tab_vocabs = train_set['source_system_tab'].unique()\n",
    "tab_map = { tab_vocabs[i]:i+1 for i in range(len(tab_vocabs))}\n",
    "train_set['source_system_tab'] = train_set['source_system_tab'].map(tab_map).fillna(3).astype(int)\n",
    "test_set['source_system_tab'] = test_set['source_system_tab'].map(tab_map).fillna(3).astype(int)\n",
    "\n",
    "screen_vocabs = train_set['source_screen_name'].unique()\n",
    "screen_map = { screen_vocabs[i]:i+1 for i in range(len(screen_vocabs))}\n",
    "train_set['source_screen_name'] = train_set['source_screen_name'].map(screen_map).fillna(3).astype(int)\n",
    "test_set['source_screen_name'] = test_set['source_screen_name'].map(screen_map).fillna(3).astype(int)\n",
    "\n",
    "sctype_vocabs = train_set['source_type'].unique()\n",
    "sctype_map = { sctype_vocabs[i]:i+1 for i in range(len(sctype_vocabs))}\n",
    "train_set['source_type'] = train_set['source_type'].map(sctype_map).fillna(3).astype(int)\n",
    "test_set['source_type'] = test_set['source_type'].map(sctype_map).fillna(3).astype(int)\n",
    "\n",
    "gid_vocabs = train_set['genre_ids'].unique()\n",
    "gid_map = { gid_vocabs[i]:i+1 for i in range(len(gid_vocabs))}\n",
    "train_set['genre_ids'] = train_set['genre_ids'].map(gid_map).fillna(3).astype(int)\n",
    "test_set['genre_ids'] = test_set['genre_ids'].map(gid_map).fillna(3).astype(int)\n",
    "\n",
    "atname_vocabs = train_set['artist_name'].unique()\n",
    "atname_map = { atname_vocabs[i]:i+1 for i in range(len(atname_vocabs))}\n",
    "train_set['artist_name'] = train_set['artist_name'].map(atname_map).fillna(3).astype(int)\n",
    "test_set['artist_name'] = test_set['artist_name'].map(atname_map).fillna(3).astype(int)\n",
    "\n",
    "gender_vocabs = train_set['gender'].unique()\n",
    "gender_map = { gender_vocabs[i]:i+1 for i in range(len(gender_vocabs))}\n",
    "train_set['gender'] = train_set['gender'].map(gender_map).fillna(3).astype(int)\n",
    "test_set['gender'] = test_set['gender'].map(gender_map).fillna(3).astype(int)\n",
    "\n",
    "via_vocabs = train_set['registered_via'].unique()\n",
    "via_map = { via_vocabs[i]:i+1 for i in range(len(via_vocabs))}\n",
    "train_set['registered_via'] = train_set['registered_via'].map(via_map).fillna(3).astype(int)\n",
    "test_set['registered_via'] = test_set['registered_via'].map(via_map).fillna(3).astype(int)\n",
    "\n",
    "city_vocabs = train_set['city'].unique()\n",
    "city_map = { city_vocabs[i]:i+1 for i in range(len(city_vocabs))}\n",
    "train_set['city'] = train_set['city'].map(city_map).fillna(3).astype(int)\n",
    "test_set['city'] = test_set['city'].map(city_map).fillna(3).astype(int)\n",
    "\n",
    "lan_vocabs = train_set['language'].unique()\n",
    "lan_map = { lan_vocabs[i]:i+1 for i in range(len(lan_vocabs))}\n",
    "train_set['language'] = train_set['language'].map(lan_map).fillna(3).astype(int)\n",
    "test_set['language'] = test_set['language'].map(lan_map).fillna(3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a7b5677ed220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Merge the training and test data with song and member data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msongs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'song_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'msno'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msongs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'song_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#Merge the training and test data with song and member data\n",
    "\n",
    "train_set = train.merge(songs, on='song_id')\n",
    "train_set = train_set.merge(members, on='msno')\n",
    "test_set = test.merge(songs, on='song_id', how='left')\n",
    "test_set = test_set.merge(members, on='msno', how='left')\n",
    "\n",
    "#Separate the submission ids from the test set\n",
    "\n",
    "ids = test_set['id']\n",
    "test_set.drop('id', axis=1, inplace=True)\n",
    "\n",
    "#Impute missing values in merged training and test sets\n",
    "\n",
    "train_set.fillna(value='unknown', axis=1, inplace=True)\n",
    "test_set.fillna(value='unknown', axis=1, inplace=True)\n",
    "\n",
    "#Impute missing song lengths with an integer value to avoid errors due to conflicting data types\n",
    "\n",
    "test_set['song_length'] = test_set['song_length'].replace('unknown', -99)\n",
    "\n",
    "msno_vocabs = train_set['msno'].unique()\n",
    "msno_map = { msno_vocabs[i]:i+1 for i in range(len(msno_vocabs))}\n",
    "train_set['msno'] = train_set['msno'].map(msno_map).fillna(3).astype(int)\n",
    "test_set['msno'] = test_set['msno'].map(msno_map).fillna(3).astype(int)\n",
    "\n",
    "song_vocabs = train_set['song_id'].unique()\n",
    "song_map = { song_vocabs[i]:i+1 for i in range(len(song_vocabs))}\n",
    "train_set['song_id'] = train_set['song_id'].map(song_map).fillna(3).astype(int)\n",
    "test_set['song_id'] = test_set['song_id'].map(song_map).fillna(3).astype(int)\n",
    "\n",
    "tab_vocabs = train_set['source_system_tab'].unique()\n",
    "tab_map = { tab_vocabs[i]:i+1 for i in range(len(tab_vocabs))}\n",
    "train_set['source_system_tab'] = train_set['source_system_tab'].map(tab_map).fillna(3).astype(int)\n",
    "test_set['source_system_tab'] = test_set['source_system_tab'].map(tab_map).fillna(3).astype(int)\n",
    "\n",
    "screen_vocabs = train_set['source_screen_name'].unique()\n",
    "screen_map = { screen_vocabs[i]:i+1 for i in range(len(screen_vocabs))}\n",
    "train_set['source_screen_name'] = train_set['source_screen_name'].map(screen_map).fillna(3).astype(int)\n",
    "test_set['source_screen_name'] = test_set['source_screen_name'].map(screen_map).fillna(3).astype(int)\n",
    "\n",
    "sctype_vocabs = train_set['source_type'].unique()\n",
    "sctype_map = { sctype_vocabs[i]:i+1 for i in range(len(sctype_vocabs))}\n",
    "train_set['source_type'] = train_set['source_type'].map(sctype_map).fillna(3).astype(int)\n",
    "test_set['source_type'] = test_set['source_type'].map(sctype_map).fillna(3).astype(int)\n",
    "\n",
    "gid_vocabs = train_set['genre_ids'].unique()\n",
    "gid_map = { gid_vocabs[i]:i+1 for i in range(len(gid_vocabs))}\n",
    "train_set['genre_ids'] = train_set['genre_ids'].map(gid_map).fillna(3).astype(int)\n",
    "test_set['genre_ids'] = test_set['genre_ids'].map(gid_map).fillna(3).astype(int)\n",
    "\n",
    "atname_vocabs = train_set['artist_name'].unique()\n",
    "atname_map = { atname_vocabs[i]:i+1 for i in range(len(atname_vocabs))}\n",
    "train_set['artist_name'] = train_set['artist_name'].map(atname_map).fillna(3).astype(int)\n",
    "test_set['artist_name'] = test_set['artist_name'].map(atname_map).fillna(3).astype(int)\n",
    "\n",
    "gender_vocabs = train_set['gender'].unique()\n",
    "gender_map = { gender_vocabs[i]:i+1 for i in range(len(gender_vocabs))}\n",
    "train_set['gender'] = train_set['gender'].map(gender_map).fillna(3).astype(int)\n",
    "test_set['gender'] = test_set['gender'].map(gender_map).fillna(3).astype(int)\n",
    "\n",
    "via_vocabs = train_set['registered_via'].unique()\n",
    "via_map = { via_vocabs[i]:i+1 for i in range(len(via_vocabs))}\n",
    "train_set['registered_via'] = train_set['registered_via'].map(via_map).fillna(3).astype(int)\n",
    "test_set['registered_via'] = test_set['registered_via'].map(via_map).fillna(3).astype(int)\n",
    "\n",
    "city_vocabs = train_set['city'].unique()\n",
    "city_map = { city_vocabs[i]:i+1 for i in range(len(city_vocabs))}\n",
    "train_set['city'] = train_set['city'].map(city_map).fillna(3).astype(int)\n",
    "test_set['city'] = test_set['city'].map(city_map).fillna(3).astype(int)\n",
    "\n",
    "lan_vocabs = train_set['language'].unique()\n",
    "lan_map = { lan_vocabs[i]:i+1 for i in range(len(lan_vocabs))}\n",
    "train_set['language'] = train_set['language'].map(lan_map).fillna(3).astype(int)\n",
    "test_set['language'] = test_set['language'].map(lan_map).fillna(3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b417c5a66b40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msplit_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_ratio\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_ratio\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "#Shuffle the data and split off 20% of the training set for use as a validation set\n",
    "\n",
    "split_ratio = 0.8\n",
    "\n",
    "train_set = train_set.sample(frac=1, random_state=6)\n",
    "val_set = train_set[int(split_ratio*train_set.shape[0]):]\n",
    "train_set = train_set[:int(split_ratio*train_set.shape[0])]\n",
    "\n",
    "#Separate the labels from the training and validation sets\n",
    "\n",
    "y_train = train_set['target']\n",
    "train_set.drop('target', axis=1, inplace=True)\n",
    "\n",
    "y_val = val_set['target']\n",
    "val_set.drop('target', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-650dce4c3126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m registered = tf.feature_column.categorical_column_with_vocabulary_list(key='registered_via',\n\u001b[0;32m---> 22\u001b[0;31m                                                                        \u001b[0mvocabulary_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'registered_via'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                                                                        \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                                        default_value=-99)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "#Designate the target feature name and the features to be used in the dataset\n",
    "\n",
    "FEATURES = ['msno', 'gender', 'city', 'bd', 'registered_via',\n",
    "            'song_id', 'artist_name', 'song_length', 'language', 'genre_ids',\n",
    "            'source_system_tab', 'source_screen_name', 'source_type', 'reg_duration']\n",
    "Types = tuple([tf.int32]*14+[tf.float32])\n",
    "LABEL = 'target'\n",
    "\n",
    "#Use the feature_column module to input each feature column into the model\n",
    "\n",
    "target = tf.feature_column.categorical_column_with_identity(key='target', num_buckets=2)\n",
    "\n",
    "length = tf.feature_column.numeric_column(key='song_length',\n",
    "                                          default_value=-1,\n",
    "                                          dtype=tf.int32)\n",
    "\n",
    "duration = tf.feature_column.numeric_column(key='reg_duration',\n",
    "                                            default_value=-1,\n",
    "                                            dtype=tf.int32)\n",
    "\n",
    "registered = tf.feature_column.categorical_column_with_vocabulary_list(key='registered_via',\n",
    "                                                                       vocabulary_list=train_set['registered_via'].unique(),\n",
    "                                                                       dtype=tf.int32,\n",
    "                                                                       default_value=-99)\n",
    "\n",
    "gender = tf.feature_column.categorical_column_with_vocabulary_list(key='gender',\n",
    "                                                                   vocabulary_list=train_set['gender'].unique(),\n",
    "                                                                   dtype=tf.int32,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "city = tf.feature_column.categorical_column_with_vocabulary_list(key='city',\n",
    "                                                          vocabulary_list=train_set['city'].unique(),\n",
    "                                                          dtype=tf.int32,\n",
    "                                                          default_value=-99)\n",
    "\n",
    "language = tf.feature_column.categorical_column_with_vocabulary_list(key='language',\n",
    "                                                                     vocabulary_list=train_set['language'].unique(),\n",
    "                                                                     dtype=tf.int32,\n",
    "                                                                     default_value=-99)\n",
    "\n",
    "artist = tf.feature_column.categorical_column_with_vocabulary_list(key='artist_name',\n",
    "                                                                   vocabulary_list=train_set['artist_name'].unique(),\n",
    "                                                                   dtype=tf.int32,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "tab = tf.feature_column.categorical_column_with_vocabulary_list(key='source_system_tab',\n",
    "                                                                vocabulary_list=train_set['source_system_tab'].unique(),\n",
    "                                                                dtype=tf.int32,\n",
    "                                                                default_value=-99)\n",
    "\n",
    "screen = tf.feature_column.categorical_column_with_vocabulary_list(key='source_screen_name',\n",
    "                                                                   vocabulary_list=train_set['source_screen_name'].unique(),\n",
    "                                                                   dtype=tf.int32,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "source = tf.feature_column.categorical_column_with_vocabulary_list(key='source_type',\n",
    "                                                                   vocabulary_list=train_set['source_type'].unique(),\n",
    "                                                                   dtype=tf.int32,\n",
    "                                                                   default_value=-99)\n",
    "\n",
    "#Bucket categorical features with many unique categories using a hash table with a size of approximately (n/0.8)*2\n",
    "\n",
    "msno = tf.feature_column.categorical_column_with_hash_bucket(key='msno',\n",
    "                                                               hash_bucket_size=90000,\n",
    "                                                               dtype=tf.int32)\n",
    "\n",
    "song_id = tf.feature_column.categorical_column_with_hash_bucket(key='song_id',\n",
    "                                                             hash_bucket_size=6000000,\n",
    "                                                             dtype=tf.int32)\n",
    "\n",
    "genre = tf.feature_column.categorical_column_with_vocabulary_list(key='genre_ids',\n",
    "                                                                  vocabulary_list=train_set['genre_ids'].unique(),\n",
    "                                                                  dtype=tf.int32,\n",
    "                                                                  default_value=-99)\n",
    "\n",
    "hashed_genre = tf.feature_column.categorical_column_with_hash_bucket(key='genre_ids',\n",
    "                                                                     hash_bucket_size=3000,\n",
    "                                                                     dtype=tf.int32)\n",
    "\n",
    "#Perform one hot encoding on categorical features with few unique values\n",
    "\n",
    "indicator_registered = tf.feature_column.indicator_column(registered)\n",
    "indicator_gender = tf.feature_column.indicator_column(gender)\n",
    "indicator_city = tf.feature_column.indicator_column(city)\n",
    "indicator_genre = tf.feature_column.indicator_column(genre)\n",
    "indicator_language = tf.feature_column.indicator_column(language)\n",
    "indicator_tab = tf.feature_column.indicator_column(tab)\n",
    "indicator_screen = tf.feature_column.indicator_column(screen)\n",
    "indicator_source = tf.feature_column.indicator_column(source)\n",
    "\n",
    "#Embed the categorical feature with <100 unique categories into dense vectors with approximately log2(n) dimensions\n",
    "\n",
    "embedded_genre = tf.feature_column.embedding_column(genre, dimension=10)\n",
    "embedded_song = tf.feature_column.embedding_column(song_id, dimension=22)\n",
    "embedded_msno = tf.feature_column.embedding_column(msno, dimension=15)\n",
    "embedded_artist = tf.feature_column.embedding_column(artist, dimension=15)\n",
    "\n",
    "#Bucket member age into age ranges, with nonsensical values going into the 0-14 or the >80 buckets\n",
    "\n",
    "age = tf.feature_column.numeric_column(key='bd',\n",
    "                                       default_value=0,\n",
    "                                       dtype=tf.int32)\n",
    "\n",
    "age_bucket = tf.feature_column.bucketized_column(age, boundaries=[0, 14, 20, 30, 40, 50, 80])\n",
    "\n",
    "#Assign features to be used in either the wide or the deep model (or both)\n",
    "\n",
    "wide_columns = []\n",
    "cross_columns = []\n",
    "deep_columns = [\n",
    "                indicator_gender, indicator_city, indicator_language, indicator_tab,\n",
    "                indicator_screen, indicator_source, indicator_registered,\n",
    "                embedded_msno, embedded_song, embedded_genre,\n",
    "                #embedded_artist,\n",
    "                duration,\n",
    "                #length, age_bucket\n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, model_type):\n",
    "    if model_type == 'wide':\n",
    "        model = tf.estimator.LinearClassifier(model_dir=model_dir,\n",
    "                                              feature_columns=wide_columns + cross_columns)\n",
    "\n",
    "    elif model_type == 'deep':\n",
    "        model = tf.estimator.DNNClassifier(model_dir=model_dir,\n",
    "                                           feature_columns=deep_columns,\n",
    "                                           hidden_units=[1024, 512, 256],\n",
    "                                           optimizer=tf.train.AdamOptimizer(learning_rate=0.001,\n",
    "                                                                            name='Adam'))\n",
    "\n",
    "    elif model_type == 'combined':\n",
    "        model = tf.estimator.DNNLinearCombinedClassifier(model_dir=model_dir,\n",
    "                                                         linear_feature_columns=cross_columns,\n",
    "                                                         dnn_feature_columns=deep_columns,\n",
    "                                                         dnn_hidden_units=[100, 50])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Methods to allow pandas.DataFrame.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.python.estimator.inputs.queues import feeding_functions\n",
    "\n",
    "try:\n",
    "  # pylint: disable=g-import-not-at-top\n",
    "  # pylint: disable=unused-import\n",
    "  import pandas as pd\n",
    "  HAS_PANDAS = True\n",
    "except IOError:\n",
    "  # Pandas writes a temporary file during import. If it fails, don't use pandas.\n",
    "  HAS_PANDAS = False\n",
    "except ImportError:\n",
    "  HAS_PANDAS = False\n",
    "\n",
    "\n",
    "def pandas_input_fn(x,\n",
    "                    y=None,\n",
    "                    batch_size=128,\n",
    "                    num_epochs=1,\n",
    "                    shuffle=None,\n",
    "                    queue_capacity=1000,\n",
    "                    num_threads=1,\n",
    "                    target_column='target'):\n",
    "  \"\"\"Returns input function that would feed Pandas DataFrame into the model.\n",
    "  Note: `y`'s index must match `x`'s index.\n",
    "  Args:\n",
    "    x: pandas `DataFrame` object.\n",
    "    y: pandas `Series` object. `None` if absent.\n",
    "    batch_size: int, size of batches to return.\n",
    "    num_epochs: int, number of epochs to iterate over data. If not `None`,\n",
    "      read attempts that would exceed this value will raise `OutOfRangeError`.\n",
    "    shuffle: bool, whether to read the records in random order.\n",
    "    queue_capacity: int, size of the read queue. If `None`, it will be set\n",
    "      roughly to the size of `x`.\n",
    "    num_threads: Integer, number of threads used for reading and enqueueing. In\n",
    "      order to have predicted and repeatable order of reading and enqueueing,\n",
    "      such as in prediction and evaluation mode, `num_threads` should be 1.\n",
    "    target_column: str, name to give the target column `y`.\n",
    "  Returns:\n",
    "    Function, that has signature of ()->(dict of `features`, `target`)\n",
    "  Raises:\n",
    "    ValueError: if `x` already contains a column with the same name as `y`, or\n",
    "      if the indexes of `x` and `y` don't match.\n",
    "    TypeError: `shuffle` is not bool.\n",
    "  \"\"\"\n",
    "  if not HAS_PANDAS:\n",
    "    raise TypeError(\n",
    "        'pandas_input_fn should not be called without pandas installed')\n",
    "\n",
    "  if not isinstance(shuffle, bool):\n",
    "    raise TypeError('shuffle must be explicitly set as boolean; '\n",
    "                    'got {}'.format(shuffle))\n",
    "\n",
    "  x = x.copy()\n",
    "  if y is not None:\n",
    "    if target_column in x:\n",
    "      raise ValueError(\n",
    "          'Cannot use name %s for target column: DataFrame already has a '\n",
    "          'column with that name: %s' % (target_column, x.columns))\n",
    "    if not np.array_equal(x.index, y.index):\n",
    "      raise ValueError('Index for x and y are mismatched.\\nIndex for x: %s\\n'\n",
    "                       'Index for y: %s\\n' % (x.index, y.index))\n",
    "    x[target_column] = y\n",
    "\n",
    "  # TODO(mdan): These are memory copies. We probably don't need 4x slack space.\n",
    "  # The sizes below are consistent with what I've seen elsewhere.\n",
    "  if queue_capacity is None:\n",
    "    if shuffle:\n",
    "      queue_capacity = 4 * len(x)\n",
    "    else:\n",
    "      queue_capacity = len(x)\n",
    "  min_after_dequeue = max(queue_capacity / 4, 1)\n",
    "\n",
    "  def input_fn():\n",
    "    \"\"\"Pandas input function.\"\"\"\n",
    "    queue = feeding_functions._enqueue_data(  # pylint: disable=protected-access\n",
    "        x,\n",
    "        queue_capacity,\n",
    "        shuffle=shuffle,\n",
    "        min_after_dequeue=min_after_dequeue,\n",
    "        num_threads=num_threads,\n",
    "        enqueue_size=batch_size,\n",
    "        num_epochs=num_epochs)\n",
    "    if num_epochs is None:\n",
    "      features = queue.dequeue_many(batch_size)\n",
    "    else:\n",
    "      features = queue.dequeue_up_to(batch_size)\n",
    "    assert len(features) == len(x.columns) + 1, ('Features should have one '\n",
    "                                                 'extra element for the index.')\n",
    "    features = features[1:]\n",
    "    features = dict(zip(list(x.columns), features))\n",
    "    if y is not None:\n",
    "      target = features.pop(target_column)\n",
    "      print(features) \n",
    "      return features, target\n",
    "    return features\n",
    "  return input_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(X, y, mode, batch_size):\n",
    "    X.fillna(value='unknown', axis=1, inplace=True)    \n",
    "\n",
    "    if mode == 'train':\n",
    "        \"\"\"return pandas_input_fn(x=pd.DataFrame({k: X[k].values for k in FEATURES}),\n",
    "                                                    y=pd.Series(y.values),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    num_epochs=None,\n",
    "                                                    shuffle=True,\n",
    "                                                    num_threads=8,\n",
    "                                                    target_column='target')\"\"\"\n",
    "        return get_train_input_fn(X,FEATURES,Types,y)\n",
    "    elif mode == 'eval':\n",
    "        return tf.estimator.inputs.pandas_input_fn(x = pd.DataFrame({k: X[k].values for k in FEATURES}),\n",
    "                                                    y = pd.Series(y.values),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    num_epochs=1,\n",
    "                                                    shuffle=False,\n",
    "                                                    num_threads=1,\n",
    "                                                    target_column='target')\n",
    "    \n",
    "    elif mode == 'predict':\n",
    "        return tf.estimator.inputs.pandas_input_fn(x=pd.DataFrame({k: X[k].values for k in FEATURES}),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    num_epochs=1,\n",
    "                                                    shuffle=False,num_threads=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_generator(Input):\n",
    "\n",
    "    def _generator():\n",
    "        for ipt in zip(*Input):\n",
    "            yield ipt\n",
    "\n",
    "    return _generator\n",
    "def get_train_input_fn(X,FEATURES,Types,Y,batchsize = 100):\n",
    "    def train_input_fn():\n",
    "        Input = [X[k].values for k in FEATURES]\n",
    "        Input.append(Y.values)\n",
    "        print(Input)\n",
    "        #Types.append(tf.float32)\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "              make_generator(Input),\n",
    "              Types)\n",
    "        dataset.repeat(None)\n",
    "        batched_dataset = dataset.batch(batchsize)\n",
    "        iterator = batched_dataset.make_one_shot_iterator()\n",
    "\n",
    "        ipt = list(iterator.get_next())\n",
    "        y = ipt[-1]\n",
    "        dic = {}\n",
    "        for i in range(len(FEATURES)):\n",
    "            ipt[i].set_shape((batchsize,))\n",
    "            dic[FEATURES[i]] = ipt[i]\n",
    "        print (dic,ipt)\n",
    "        y.set_shape((batchsize,))\n",
    "        return dic, tf.expand_dims(y,axis=1)\n",
    "    return train_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model_dir, model_type, train_steps, X_train, y_train, X_test, y_test, batch_size):\n",
    "\n",
    "#Create a temporary directory to store the model if no model directory argument is given\n",
    "\n",
    "    model_dir = tempfile.mkdtemp() if not model_dir else model_dir\n",
    "    \n",
    "    print('build_estimator')\n",
    "    model = build_estimator(model_dir, model_type)\n",
    "    \n",
    "    print('train start')\n",
    "    \n",
    "    model.train(input_fn=input_fn(X_train, y_train, mode='train', batch_size=batch_size),\n",
    "                max_steps=train_steps)\n",
    "    \n",
    "#Evaluate the trained model on a separate validation set in n/batch_size steps\n",
    "    \n",
    "    model.evaluate(input_fn=input_fn(X_test, y_test, mode='eval', batch_size=batch_size),\n",
    "                        steps=(X_test.shape[0]//batch_size + 1))\n",
    "\n",
    "    print('end!')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fe7254d06433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m deep_model = train_model(model_dir='model/', model_type='deep', train_steps=600000,\n\u001b[0;32m----> 2\u001b[0;31m                          \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                          \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          batch_size=100)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "deep_model = train_model(model_dir='model/', model_type='deep', train_steps=600000,\n",
    "                         X_train=train_set, y_train=y_train,\n",
    "                         X_test=val_set, y_test=y_val,\n",
    "                         batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deep_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5e85d8f2f907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m results = deep_model.evaluate(input_fn=input_fn(train_set, y_train, mode='eval', batch_size=1000),\n\u001b[0m\u001b[1;32m      2\u001b[0m                               steps=1476)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deep_model' is not defined"
     ]
    }
   ],
   "source": [
    "results = deep_model.evaluate(input_fn=input_fn(train_set, y_train, mode='eval', batch_size=1000),\n",
    "                              steps=1476)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deep_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-20cc9a473caf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m predictions = deep_model.predict(input_fn=input_fn(test_set, None, mode='predict',\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                    batch_size=10000))\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deep_model' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = deep_model.predict(input_fn=input_fn(test_set, None, mode='predict',\n",
    "                                                   batch_size=10000))\n",
    "\n",
    "submission = list()\n",
    "\n",
    "for row in predictions:\n",
    "    submission.append(row['probabilities'][1])\n",
    "\n",
    "pd.DataFrame(data={'id': ids,\n",
    "                   'target': np.array(submission)}).to_csv('submissions/embedded_deep.csv',\n",
    "                                                           header=['id', 'target'],\n",
    "                                                           index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
